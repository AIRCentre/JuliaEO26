{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Score-Based Generative Modeling in 2D\n",
        "\n",
        "## Tutorial: Learning Score Functions with Denoising Score Matching\n",
        "\n",
        "This notebook demonstrates **score-based generative modeling** using Julia, a powerful approach for learning probability distributions and generating samples. We'll work with a 2D example to visualize the concepts clearly.\n",
        "\n",
        "### What You'll Learn\n",
        "1. What is a **score function** and why it's useful\n",
        "2. The **denoising score matching** objective\n",
        "3. How to train a neural network to approximate scores\n",
        "4. How to generate samples using **Langevin dynamics**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies\n",
        "\n",
        "This tutorial requires the following Julia packages:\n",
        "\n",
        "| Package | Purpose |\n",
        "|---------|----------|\n",
        "| `GLMakie` | Interactive plotting and visualization |\n",
        "| `LinearAlgebra` | Matrix operations and linear algebra |\n",
        "| `ProgressBars` | Training progress visualization |\n",
        "| `Random` | Random number generation |\n",
        "| `Statistics` | Statistical functions (mean, cov, etc.) |\n",
        "| `HDF5` | Reading data from HDF5 files |\n",
        "| `Enzyme` | Automatic differentiation for gradients |\n",
        "\n",
        "Make sure to activate the project environment before running:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "using GLMakie, LinearAlgebra, ProgressBars, Random, Statistics, HDF5\n",
        "\n",
        "Random.seed!(1234)  # For reproducibility\n",
        "\n",
        "# Include custom neural network implementations\n",
        "include(\"simple_networks.jl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: Understanding Score Functions\n",
        "\n",
        "### What is a Score Function?\n",
        "\n",
        "For a probability distribution $p(\\mathbf{x})$, the **score function** is defined as:\n",
        "\n",
        "$$\\mathbf{s}(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$$\n",
        "\n",
        "The score tells us the direction of steepest increase in log-probability. In other words, it points toward regions of higher probability density.\n",
        "\n",
        "### Why Use Scores?\n",
        "\n",
        "1. **Avoids normalization**: Unlike $p(\\mathbf{x})$, the score doesn't require computing the partition function $Z = \\int p(\\mathbf{x}) d\\mathbf{x}$\n",
        "2. **Enables sampling**: We can generate samples using Langevin dynamics\n",
        "3. **Foundation for diffusion models**: Modern generative AI (DALL-E, Stable Diffusion) builds on these ideas\n",
        "\n",
        "### Our Target Distribution\n",
        "\n",
        "We'll work with a 2D distribution defined by the potential energy function:\n",
        "\n",
        "$$V(\\mathbf{x}) = \\frac{(x_1^2 - 1)^2}{4} + \\frac{(x_2^2 - 1)^2}{4} + \\frac{x_1 x_2}{3}$$\n",
        "\n",
        "The probability distribution is:\n",
        "$$p(\\mathbf{x}) \\propto \\exp(-V(\\mathbf{x}))$$\n",
        "\n",
        "This creates a **bimodal distribution** with two peaks (modes) due to the $(x^2 - 1)^2$ terms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Score Functions (2D)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "\"\"\"\n",
        "Exact score function from the potential V(x) = (xâ‚Â²-1)Â²/4 + (xâ‚‚Â²-1)Â²/4 + xâ‚xâ‚‚/3\n",
        "\n",
        "For p(x) âˆ exp(-V(x)), the score is:\n",
        "    s(x) = âˆ‡log p(x) = -âˆ‡V(x)\n",
        "\n",
        "Computing âˆ‡V:\n",
        "    âˆ‚V/âˆ‚xâ‚ = xâ‚Â³ - xâ‚ + xâ‚‚/3\n",
        "    âˆ‚V/âˆ‚xâ‚‚ = xâ‚‚Â³ - xâ‚‚ + xâ‚/3\n",
        "\n",
        "Therefore:\n",
        "    s(x) = [xâ‚ - xâ‚Â³ - xâ‚‚/3, xâ‚‚ - xâ‚‚Â³ - xâ‚/3]\n",
        "\"\"\"\n",
        "function exact_score(x)\n",
        "    x1, x2 = x[1], x[2]\n",
        "    return [x1 - x1^3 - x2/3, x2 - x2^3 - x1/3]\n",
        "end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussian Mixture Score (Empirical)\n",
        "\n",
        "When we only have samples from the distribution (no analytical form), we can estimate the score using a **kernel density estimate** with Gaussian kernels:\n",
        "\n",
        "$$\\hat{p}(\\mathbf{x}) = \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{N}(\\mathbf{x} | \\mathbf{x}_i, \\sigma^2 I)$$\n",
        "\n",
        "The score of this mixture is:\n",
        "\n",
        "$$\\mathbf{s}_{\\text{GM}}(\\mathbf{x}) = \\frac{\\sum_{i=1}^{M} w_i (\\mathbf{x}_i - \\mathbf{x})}{\\sigma^2}$$\n",
        "\n",
        "where $w_i \\propto \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_i\\|^2}{2\\sigma^2}\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Gaussian mixture score estimator.\n",
        "\n",
        "This computes the score of a kernel density estimate formed by placing\n",
        "Gaussian kernels at each data point.\n",
        "\n",
        "Arguments:\n",
        "    x     : 2-element vector (query point)\n",
        "    data  : 2Ã—M matrix (data samples)\n",
        "    sigma : kernel bandwidth\n",
        "\"\"\"\n",
        "function gaussian_mixture_score(x, data, sigma)\n",
        "    m = size(data, 2)\n",
        "    score_value = zeros(2)\n",
        "    denominator = 0.0\n",
        "    \n",
        "    for i in 1:m\n",
        "        Î” = data[:, i] .- x\n",
        "        U = exp(-(0.5 / sigma^2) * dot(Î”, Î”))\n",
        "        score_value .+= U .* Î”\n",
        "        denominator += U\n",
        "    end\n",
        "    \n",
        "    return score_value ./ (denominator * sigma^2)\n",
        "end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Denoising Score Matching\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "We want to train a neural network $\\mathbf{s}_\\theta(\\mathbf{x})$ to approximate the true score $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$.\n",
        "\n",
        "**Naive approach (doesn't work):**\n",
        "$$\\mathcal{L}_{\\text{naive}} = \\mathbb{E}_{p(\\mathbf{x})} \\left[ \\| \\mathbf{s}_\\theta(\\mathbf{x}) - \\nabla_\\mathbf{x} \\log p(\\mathbf{x}) \\|^2 \\right]$$\n",
        "\n",
        "This requires knowing $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$ â€” but that's exactly what we're trying to learn!\n",
        "\n",
        "### The Denoising Score Matching Trick\n",
        "\n",
        "**Key insight**: Instead of matching the score of the data distribution, we match the score of a **noise-perturbed** distribution!\n",
        "\n",
        "Given a data point $\\mathbf{x}$, we create a noisy version:\n",
        "$$\\tilde{\\mathbf{x}} = \\mathbf{x} + \\sigma \\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(0, I)$$\n",
        "\n",
        "The score of the conditional distribution $p(\\tilde{\\mathbf{x}} | \\mathbf{x})$ is:\n",
        "$$\\nabla_{\\tilde{\\mathbf{x}}} \\log p(\\tilde{\\mathbf{x}} | \\mathbf{x}) = -\\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma^2} = -\\frac{\\mathbf{z}}{\\sigma}$$\n",
        "\n",
        "### Denoising Score Matching Loss\n",
        "\n",
        "The **denoising score matching** objective is:\n",
        "\n",
        "$$\\boxed{\\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x})} \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(0, I)} \\left[ \\left\\| \\mathbf{s}_\\theta(\\mathbf{x} + \\sigma\\mathbf{z}) + \\frac{\\mathbf{z}}{\\sigma} \\right\\|^2 \\right]}$$\n",
        "\n",
        "**Why this works:**\n",
        "- We know $\\mathbf{z}$ (we sampled it!)\n",
        "- The network learns to predict $-\\mathbf{z}/\\sigma$, which is the score of the noisy distribution\n",
        "- As $\\sigma \\to 0$, this converges to the true data score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Loss Functions (2D)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "\"\"\"\n",
        "Gaussian mixture loss function.\n",
        "\n",
        "Trains the network to match the Gaussian mixture score estimate.\n",
        "This is more expensive but provides a ground truth to compare against.\n",
        "\"\"\"\n",
        "function gaussian_mixture_loss_function(model, data, Ïƒ, zs)\n",
        "    batchsize = size(data, 2)\n",
        "    lossval = [0.0]\n",
        "    \n",
        "    for i in 1:batchsize\n",
        "        x = data[:, i] .+ Ïƒ .* zs[:, i]\n",
        "        Å· = model(x)\n",
        "        y = gaussian_mixture_score(x, data, Ïƒ)\n",
        "        lossval[1] += sum((y .- Å·).^2) / batchsize\n",
        "    end\n",
        "    \n",
        "    return lossval[1]\n",
        "end\n",
        "\n",
        "\"\"\"\n",
        "Denoising Score Matching loss function.\n",
        "\n",
        "This is the KEY loss function:\n",
        "    L = E[(s_Î¸(xÌƒ) + z/Ïƒ)Â²]\n",
        "    \n",
        "where xÌƒ = x + Ïƒz and z ~ N(0, I)\n",
        "\n",
        "The network learns to predict -z/Ïƒ, which equals the score of the\n",
        "noise-perturbed distribution.\n",
        "\"\"\"\n",
        "function denoising_loss_function(model, data, sigma, noises)\n",
        "    batchsize = size(data, 2)\n",
        "    lossval = [0.0]\n",
        "    \n",
        "    for i in 1:batchsize\n",
        "        x = data[:, i]           # Original data point\n",
        "        z = noises[:, i]         # Random noise\n",
        "        xÌƒ = x .+ sigma .* z     # Perturbed point\n",
        "        Å· = model(xÌƒ)            # Network prediction\n",
        "        \n",
        "        # Loss: (prediction + z/Ïƒ)Â²\n",
        "        # Network should predict -z/Ïƒ\n",
        "        lossval[1] += sum((Å· .+ z ./ sigma).^2) / batchsize\n",
        "    end\n",
        "    \n",
        "    return lossval[1]\n",
        "end\n",
        "\n",
        "# Use denoising score matching for training\n",
        "loss_function = denoising_loss_function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Loading and Visualizing the Data\n",
        "\n",
        "We load samples from our target distribution, generated using Langevin dynamics on the potential $V(\\mathbf{x})$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Load Data and Setup\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Load 2D data from HDF5 file\n",
        "data = h5read(\"potential_data_2D.hdf5\", \"data\")  # 2Ã—M matrix\n",
        "M = size(data, 2)\n",
        "println(\"Loaded $(M) samples of 2D data\")\n",
        "\n",
        "# Noise level for score matching\n",
        "sigma = Ïƒ = 0.05\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 1: Data Distribution\n",
        "\n",
        "This figure shows the **marginal distributions** of our 2D data along each axis.\n",
        "\n",
        "**What to observe:**\n",
        "- **Bimodal structure**: Each dimension shows two peaks around $x \\approx \\pm 1$\n",
        "- This comes from the $(x^2 - 1)^2$ terms in the potential, which have minima at $x = \\pm 1$\n",
        "- The coupling term $x_1 x_2 / 3$ creates correlation between the dimensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# Visualize data distribution\n",
        "fig = Figure(size=(800, 400))\n",
        "ax1 = Axis(fig[1, 1], title=\"xâ‚ Distribution\", xlabel=\"xâ‚\", ylabel=\"Density\")\n",
        "hist!(ax1, data[1, :], bins=100, normalization=:pdf, color=:steelblue)\n",
        "ax2 = Axis(fig[1, 2], title=\"xâ‚‚ Distribution\", xlabel=\"xâ‚‚\", ylabel=\"Density\")\n",
        "hist!(ax2, data[2, :], bins=100, normalization=:pdf, color=:steelblue)\n",
        "display(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# Test score computation on a grid\n",
        "test_point = [0.0, 0.0]\n",
        "tmp = gaussian_mixture_score(test_point, data, sigma)\n",
        "println(\"Gaussian mixture score at origin: \", tmp)\n",
        "println(\"Exact score at origin: \", exact_score(test_point))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Neural Network Architecture\n",
        "\n",
        "We use a simple feedforward neural network with:\n",
        "- **Input**: 2D position $\\mathbf{x}$\n",
        "- **Hidden layer**: 50 neurons with Swish activation\n",
        "- **Linear bypass**: A direct linear connection from input to output\n",
        "- **Output**: 2D score estimate $\\mathbf{s}_\\theta(\\mathbf{x})$\n",
        "\n",
        "The **linear bypass** helps the network learn linear score components directly, which is important since the score has both linear and nonlinear terms:\n",
        "\n",
        "$$\\mathbf{s}(\\mathbf{x}) = \\begin{bmatrix} x_1 - x_1^3 - x_2/3 \\\\ x_2 - x_2^3 - x_1/3 \\end{bmatrix} = \\underbrace{\\begin{bmatrix} x_1 - x_2/3 \\\\ x_2 - x_1/3 \\end{bmatrix}}_{\\text{linear}} + \\underbrace{\\begin{bmatrix} -x_1^3 \\\\ -x_2^3 \\end{bmatrix}}_{\\text{nonlinear}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Define Network\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "NÎ¸ = 2        # Input dimension (2D position)\n",
        "NÎ¸á´´ = 50      # Hidden dimension\n",
        "Nout = 2      # Output dimension (2D score)\n",
        "\n",
        "# Create network with linear bypass for better learning\n",
        "network = OneLayerNetworkWithLinearByPass(NÎ¸, Nout, NÎ¸á´´)\n",
        "dnetwork = deepcopy(network)           # For storing gradients\n",
        "smoothed_network = deepcopy(network)   # For exponential moving average\n",
        "\n",
        "println(\"Network architecture: $(NÎ¸) â†’ $(NÎ¸á´´) â†’ $(Nout) with linear bypass\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# Test loss computation\n",
        "test_data = data[:, 1:10]\n",
        "test_noise = randn(2, 10)\n",
        "initial_loss = loss_function(network, test_data, Ïƒ, test_noise)\n",
        "println(\"Initial loss on test batch: $(round(initial_loss, digits=3))\")\n",
        "\n",
        "# Create convenience functions for score computation\n",
        "gmscore(x) = gaussian_mixture_score(x, data, sigma)\n",
        "exactscore(x) = exact_score(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: Training the Network\n",
        "\n",
        "We train using:\n",
        "- **Adam optimizer**: Adaptive learning rates for each parameter\n",
        "- **Mini-batch gradient descent**: Process data in batches of 20 samples\n",
        "- **Enzyme.jl**: Automatic differentiation for computing gradients\n",
        "- **Exponential moving average**: Smooth network parameters for stability\n",
        "\n",
        "### Training Loop Details\n",
        "\n",
        "For each epoch:\n",
        "1. Shuffle training data into mini-batches\n",
        "2. For each batch:\n",
        "   - Sample fresh noise $\\mathbf{z}$\n",
        "   - Compute denoising loss\n",
        "   - Backpropagate and update weights\n",
        "3. Evaluate loss on held-out test data\n",
        "4. Update exponential moving average of weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Training\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "adam = Adam(network)          # Initialize Adam optimizer\n",
        "batchsize = 20                # Samples per batch\n",
        "loss_list = Float64[]         # Track training loss\n",
        "test_loss_list = Float64[]    # Track test loss\n",
        "epochs = 200                  # Number of training epochs\n",
        "network_parameters = copy(parameters(network))  # For EMA smoothing\n",
        "\n",
        "println(\"Starting training for $(epochs) epochs...\")\n",
        "println(\"Batch size: $(batchsize), Total samples: $(M)\")\n",
        "\n",
        "for i in ProgressBar(1:epochs)\n",
        "    # Split data: odd indices for training, even for testing\n",
        "    shuffled_list = chunk_list(shuffle(1:2:M), batchsize)\n",
        "    shuffled_test_list = chunk_list(shuffle(2:2:M), batchsize)\n",
        "    loss_value = 0.0\n",
        "    N = length(shuffled_list)\n",
        "    \n",
        "    # â”€â”€â”€ Training Pass â”€â”€â”€\n",
        "    for permuted_list in shuffled_list\n",
        "        Î¸batch = data[:, permuted_list]  # 2Ã—batchsize matrix\n",
        "        zero!(dnetwork)                   # Reset gradients\n",
        "        zs = randn(2, length(permuted_list))  # Sample fresh noise\n",
        "        \n",
        "        # Compute gradients via Enzyme autodiff\n",
        "        autodiff(Enzyme.Reverse, loss_function, Active, \n",
        "                 DuplicatedNoNeed(network, dnetwork), \n",
        "                 Const(Î¸batch), Const(sigma), Const(zs))\n",
        "        \n",
        "        # Update network with Adam\n",
        "        update!(adam, network, dnetwork)\n",
        "        loss_value += loss_function(network, Î¸batch, sigma, zs) / N\n",
        "    end\n",
        "    push!(loss_list, loss_value)\n",
        "    \n",
        "    # â”€â”€â”€ Test Pass â”€â”€â”€\n",
        "    loss_value = 0.0\n",
        "    N = length(shuffled_test_list)\n",
        "    for permuted_list in shuffled_test_list\n",
        "        Î¸batch = data[:, permuted_list]\n",
        "        zs = randn(2, length(permuted_list))\n",
        "        loss_value += loss_function(network, Î¸batch, sigma, zs) / N\n",
        "    end\n",
        "    push!(test_loss_list, loss_value)\n",
        "    \n",
        "    # â”€â”€â”€ Exponential Moving Average â”€â”€â”€\n",
        "    m = 0.9  # Smoothing factor\n",
        "    network_parameters .= m * network_parameters + (1 - m) * parameters(network)\n",
        "    set_parameters!(smoothed_network, network_parameters)\n",
        "end\n",
        "\n",
        "println(\"\\nTraining complete!\")\n",
        "println(\"Final training loss: $(round(loss_list[end], digits=4))\")\n",
        "println(\"Final test loss: $(round(test_loss_list[end], digits=4))\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 2: Training Loss Curves\n",
        "\n",
        "This figure shows the **learning dynamics** during training.\n",
        "\n",
        "**What to observe:**\n",
        "- **Log-scale y-axis**: Helps visualize orders of magnitude improvement\n",
        "- **Training loss (blue)**: Should decrease steadily\n",
        "- **Test loss (red)**: Should follow training loss closely\n",
        "- **Convergence**: Both losses stabilize after sufficient epochs\n",
        "- **No overfitting**: If test loss diverges from training loss, the model is overfitting\n",
        "\n",
        "**Note**: The denoising loss has an irreducible minimum â€” even the perfect score function has nonzero loss due to the stochastic noise $\\mathbf{z}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Plot Training Loss\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "loss_fig = Figure(size=(700, 400))\n",
        "ax = Axis(loss_fig[1, 1]; \n",
        "          title=\"Training Progress\", \n",
        "          xlabel=\"Epoch\", \n",
        "          ylabel=\"Logâ‚â‚€(Loss)\")\n",
        "scatter!(ax, log10.(loss_list); color=:steelblue, markersize=6, label=\"Training Loss\")\n",
        "scatter!(ax, log10.(test_loss_list); color=:crimson, markersize=6, label=\"Test Loss\")\n",
        "axislegend(ax, position=:rt)\n",
        "display(loss_fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: Evaluating the Learned Score\n",
        "\n",
        "We compare three score functions:\n",
        "1. **Network**: Our trained neural network $\\mathbf{s}_\\theta$\n",
        "2. **Gaussian Mixture**: The kernel density estimate score\n",
        "3. **Exact**: The analytical score from $-\\nabla V$\n",
        "\n",
        "Lower loss indicates better score estimation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Evaluate Final Losses\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "\"\"\"\n",
        "Evaluate denoising loss for any score function.\n",
        "This allows us to compare different score estimators on the same footing.\n",
        "\"\"\"\n",
        "function denoising_loss_with_fn(score_fn, data, Ïƒ, noises)\n",
        "    batchsize = size(data, 2)\n",
        "    lossval = 0.0\n",
        "    \n",
        "    for i in 1:batchsize\n",
        "        x = data[:, i]\n",
        "        z = noises[:, i]\n",
        "        xÌƒ = x .+ Ïƒ .* z\n",
        "        Å· = score_fn(xÌƒ)\n",
        "        lossval += sum((Å· .+ z ./ Ïƒ).^2) / batchsize\n",
        "    end\n",
        "    \n",
        "    return lossval\n",
        "end\n",
        "\n",
        "# Define score functions for evaluation\n",
        "network_score_fn(x) = predict(network, x)\n",
        "\n",
        "# Average over multiple noise realizations for stable estimates\n",
        "skip = 50\n",
        "number_of_samples = 10\n",
        "\n",
        "println(\"Evaluating score functions (averaging over $(number_of_samples) noise samples)...\")\n",
        "\n",
        "l1 = mean([denoising_loss_with_fn(network_score_fn, data[:, 1:skip:end], Ïƒ, randn(2, M)) \n",
        "           for _ in 1:number_of_samples])\n",
        "l2 = mean([denoising_loss_with_fn(gmscore, data[:, 1:skip:end], Ïƒ, randn(2, M)) \n",
        "           for _ in ProgressBar(1:number_of_samples)])\n",
        "l3 = mean([denoising_loss_with_fn(exactscore, data[:, 1:skip:end], Ïƒ, randn(2, M)) \n",
        "           for _ in 1:number_of_samples])\n",
        "\n",
        "println(\"\\n\" * \"=\"^50)\n",
        "println(\"SCORE FUNCTION COMPARISON\")\n",
        "println(\"=\"^50)\n",
        "println(\"Network Loss:          $(round(l1, digits=2))\")\n",
        "println(\"Gaussian Mixture Loss: $(round(l2, digits=2))\")\n",
        "println(\"Exact Score Loss:      $(round(l3, digits=2))\")\n",
        "println(\"=\"^50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7: Visualizing the Learned Score Field\n",
        "\n",
        "### Figure 3: Score Vector Fields\n",
        "\n",
        "This figure compares the **score vector fields** from all three methods.\n",
        "\n",
        "**How to read these plots:**\n",
        "- **Arrows**: Point in the direction of increasing probability\n",
        "- **Arrow direction**: The score direction $\\mathbf{s}(\\mathbf{x}) / \\|\\mathbf{s}(\\mathbf{x})\\|$\n",
        "- **Gray dots**: Subsampled data points showing the true distribution\n",
        "\n",
        "**What to observe:**\n",
        "- All three methods should show arrows pointing **toward the modes** (high-density regions)\n",
        "- Near the origin, arrows point outward (low probability region between modes)\n",
        "- Near $(\\pm 1, \\pm 1)$, arrows point inward (high probability modes)\n",
        "- The network should approximate the exact score pattern\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Visualize Learned Score Field\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Create grid for visualization\n",
        "x_range = range(-2, 2, length=30)\n",
        "y_range = range(-2, 2, length=30)\n",
        "\n",
        "# Preallocate score arrays\n",
        "network_scores_u = zeros(length(x_range), length(y_range))\n",
        "network_scores_v = zeros(length(x_range), length(y_range))\n",
        "gm_scores_u = zeros(length(x_range), length(y_range))\n",
        "gm_scores_v = zeros(length(x_range), length(y_range))\n",
        "exact_scores_u = zeros(length(x_range), length(y_range))\n",
        "exact_scores_v = zeros(length(x_range), length(y_range))\n",
        "\n",
        "println(\"Computing score fields on grid...\")\n",
        "for (i, x) in ProgressBar(enumerate(x_range))\n",
        "    for (j, y) in enumerate(y_range)\n",
        "        # Network score\n",
        "        net_score = predict(network, [x, y])\n",
        "        network_scores_u[i, j] = net_score[1]\n",
        "        network_scores_v[i, j] = net_score[2]\n",
        "        \n",
        "        # Gaussian mixture score\n",
        "        gm_score = gaussian_mixture_score([x, y], data, sigma)\n",
        "        gm_scores_u[i, j] = gm_score[1]\n",
        "        gm_scores_v[i, j] = gm_score[2]\n",
        "        \n",
        "        # Exact score\n",
        "        exact_score_val = exact_score([x, y])\n",
        "        exact_scores_u[i, j] = exact_score_val[1]\n",
        "        exact_scores_v[i, j] = exact_score_val[2]\n",
        "    end\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# Normalize vectors for uniform arrow lengths (shows direction only)\n",
        "function normalize_vectors(u, v, scale=0.15)\n",
        "    mag = sqrt.(u.^2 .+ v.^2)\n",
        "    mag[mag .< 1e-10] .= 1e-10  # Avoid division by zero\n",
        "    return u .* scale ./ mag, v .* scale ./ mag\n",
        "end\n",
        "\n",
        "net_u_scaled, net_v_scaled = normalize_vectors(network_scores_u, network_scores_v)\n",
        "gm_u_scaled, gm_v_scaled = normalize_vectors(gm_scores_u, gm_scores_v)\n",
        "exact_u_scaled, exact_v_scaled = normalize_vectors(exact_scores_u, exact_scores_v)\n",
        "\n",
        "# Create grid coordinate matrices\n",
        "x_grid = [x for x in x_range, y in y_range]\n",
        "y_grid = [y for x in x_range, y in y_range]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# Plot score vector field comparison\n",
        "fig = Figure(size=(1800, 600))\n",
        "\n",
        "# Network score field\n",
        "ax1 = Axis(fig[1, 1], \n",
        "           title=\"Network Score Field\\n(Loss: $(round(l1, digits=1)))\", \n",
        "           xlabel=\"xâ‚\", ylabel=\"xâ‚‚\", \n",
        "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
        "scatter!(ax1, data[1, 1:100:end], data[2, 1:100:end], \n",
        "         markersize=3, alpha=0.3, color=:gray)\n",
        "arrows!(ax1, vec(x_grid), vec(y_grid), vec(net_u_scaled), vec(net_v_scaled), \n",
        "        color=:steelblue, linewidth=1.5, arrowsize=8)\n",
        "\n",
        "# Gaussian mixture score field\n",
        "ax2 = Axis(fig[1, 2], \n",
        "           title=\"Gaussian Mixture Score Field\\n(Loss: $(round(l2, digits=1)))\", \n",
        "           xlabel=\"xâ‚\", ylabel=\"xâ‚‚\", \n",
        "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
        "scatter!(ax2, data[1, 1:100:end], data[2, 1:100:end], \n",
        "         markersize=3, alpha=0.3, color=:gray)\n",
        "arrows!(ax2, vec(x_grid), vec(y_grid), vec(gm_u_scaled), vec(gm_v_scaled), \n",
        "        color=:crimson, linewidth=1.5, arrowsize=8)\n",
        "\n",
        "# Exact score field\n",
        "ax3 = Axis(fig[1, 3], \n",
        "           title=\"Exact Score Field\\n(Loss: $(round(l3, digits=1)))\", \n",
        "           xlabel=\"xâ‚\", ylabel=\"xâ‚‚\", \n",
        "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
        "scatter!(ax3, data[1, 1:100:end], data[2, 1:100:end], \n",
        "         markersize=3, alpha=0.3, color=:gray)\n",
        "arrows!(ax3, vec(x_grid), vec(y_grid), vec(exact_u_scaled), vec(exact_v_scaled), \n",
        "        color=:forestgreen, linewidth=1.5, arrowsize=8)\n",
        "\n",
        "display(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 4: Score Magnitude Heatmaps\n",
        "\n",
        "This figure shows the **magnitude** of the score at each point: $\\|\\mathbf{s}(\\mathbf{x})\\|$.\n",
        "\n",
        "**What to observe:**\n",
        "- **Low magnitude** (dark) near modes: At high probability regions, the score is small\n",
        "- **High magnitude** (bright) away from modes: The score pushes samples toward high-density regions\n",
        "- **Saddle point at origin**: The score has a critical point at $(0, 0)$\n",
        "- **Symmetry**: The pattern should be symmetric due to the symmetric potential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Plot Score Magnitude Comparison\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Compute score magnitudes\n",
        "network_mag = sqrt.(network_scores_u.^2 .+ network_scores_v.^2)\n",
        "gm_mag = sqrt.(gm_scores_u.^2 .+ gm_scores_v.^2)\n",
        "exact_mag = sqrt.(exact_scores_u.^2 .+ exact_scores_v.^2)\n",
        "\n",
        "# Common color scale based on exact score\n",
        "cmin, cmax = quantile(vec(exact_mag), [0.01, 0.99])\n",
        "\n",
        "fig_mag = Figure(size=(1800, 500))\n",
        "\n",
        "ax1 = Axis(fig_mag[1, 1], title=\"Network Score Magnitude\", \n",
        "           xlabel=\"xâ‚\", ylabel=\"xâ‚‚\", \n",
        "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
        "hm1 = heatmap!(ax1, x_range, y_range, network_mag, \n",
        "               colormap=:viridis, colorrange=(0, cmax))\n",
        "\n",
        "ax2 = Axis(fig_mag[1, 2], title=\"Gaussian Mixture Score Magnitude\", \n",
        "           xlabel=\"xâ‚\", ylabel=\"xâ‚‚\", \n",
        "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
        "hm2 = heatmap!(ax2, x_range, y_range, gm_mag, \n",
        "               colormap=:viridis, colorrange=(0, cmax))\n",
        "\n",
        "ax3 = Axis(fig_mag[1, 3], title=\"Exact Score Magnitude\", \n",
        "           xlabel=\"xâ‚\", ylabel=\"xâ‚‚\", \n",
        "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
        "hm3 = heatmap!(ax3, x_range, y_range, exact_mag, \n",
        "               colormap=:viridis, colorrange=(0, cmax))\n",
        "\n",
        "Colorbar(fig_mag[1, 4], hm3, label=\"Score Magnitude ||s(x)||\")\n",
        "\n",
        "display(fig_mag)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 8: Sampling with Langevin Dynamics\n",
        "\n",
        "Now we use our learned score to **generate new samples** from the distribution!\n",
        "\n",
        "### Langevin Dynamics\n",
        "\n",
        "Given the score function $\\mathbf{s}(\\mathbf{x})$, we can sample from $p(\\mathbf{x})$ using the **overdamped Langevin SDE**:\n",
        "\n",
        "$$d\\mathbf{x}_t = \\mathbf{s}(\\mathbf{x}_t) dt + \\sqrt{2} \\, d\\mathbf{W}_t$$\n",
        "\n",
        "Discretized with time step $\\Delta t$:\n",
        "\n",
        "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\Delta t \\cdot \\mathbf{s}(\\mathbf{x}_t) + \\sqrt{2 \\Delta t} \\cdot \\boldsymbol{\\xi}_t$$\n",
        "\n",
        "where $\\boldsymbol{\\xi}_t \\sim \\mathcal{N}(0, I)$.\n",
        "\n",
        "### Runge-Kutta Integration\n",
        "\n",
        "We use **4th-order Runge-Kutta (RK4)** for the deterministic part to improve accuracy:\n",
        "\n",
        "$$\\mathbf{k}_1 = \\mathbf{s}(\\mathbf{x}_t)$$\n",
        "$$\\mathbf{k}_2 = \\mathbf{s}(\\mathbf{x}_t + \\frac{\\Delta t}{2} \\mathbf{k}_1)$$\n",
        "$$\\mathbf{k}_3 = \\mathbf{s}(\\mathbf{x}_t + \\frac{\\Delta t}{2} \\mathbf{k}_2)$$\n",
        "$$\\mathbf{k}_4 = \\mathbf{s}(\\mathbf{x}_t + \\Delta t \\cdot \\mathbf{k}_3)$$\n",
        "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\frac{\\Delta t}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4) + \\sqrt{2 \\Delta t} \\cdot \\boldsymbol{\\xi}_t$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Sampling from the Learned Score Field\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# Use network as our score function\n",
        "âˆ‡V(x) = predict(network, x)\n",
        "\n",
        "# Langevin dynamics parameters\n",
        "Ïµ = sqrt(2)    # Noise coefficient (âˆš2 for standard Langevin)\n",
        "Nâ‚œ = 1000      # Number of time steps\n",
        "Nâ‚‘ = 1000      # Number of ensemble members (parallel chains)\n",
        "Î”t = 0.1       # Time step size\n",
        "\n",
        "# Initialize from standard Gaussian\n",
        "xâ‚€ = randn(2, Nâ‚‘)\n",
        "\n",
        "println(\"Running Langevin dynamics...\")\n",
        "println(\"  Samples: $(Nâ‚‘), Steps: $(Nâ‚œ), Î”t: $(Î”t)\")\n",
        "\n",
        "for t in ProgressBar(1:Nâ‚œ)\n",
        "    ğ’© = randn(2, Nâ‚‘)  # Fresh noise for all samples\n",
        "    \n",
        "    for Ï‰ in 1:Nâ‚‘\n",
        "        x = xâ‚€[:, Ï‰]\n",
        "        \n",
        "        # Runge-Kutta 4 stages for deterministic drift\n",
        "        k1 = âˆ‡V(x)\n",
        "        k2 = âˆ‡V(x + Î”t / 2 * k1)\n",
        "        k3 = âˆ‡V(x + Î”t / 2 * k2)\n",
        "        k4 = âˆ‡V(x + Î”t * k3)\n",
        "        \n",
        "        # RK4 update (deterministic part)\n",
        "        xâ‚€[:, Ï‰] .= x + Î”t / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "        \n",
        "        # Add stochastic noise (Brownian motion)\n",
        "        xâ‚€[:, Ï‰] .+= Ïµ * âˆšÎ”t * ğ’©[:, Ï‰]\n",
        "    end\n",
        "end\n",
        "\n",
        "println(\"\\nSampling complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Figure 5: Generated Samples vs Original Data\n",
        "\n",
        "This figure compares the **marginal distributions** of our generated samples (blue) with the original training data (red).\n",
        "\n",
        "**What to observe:**\n",
        "- **Mode locations**: Generated samples should cluster around $x \\approx \\pm 1$\n",
        "- **Mode weights**: The relative heights of the two peaks should match\n",
        "- **Shape**: The overall distribution shape should be similar\n",
        "- **Quality indicator**: Good overlap means the network learned the distribution well\n",
        "\n",
        "**Common issues:**\n",
        "- Missing modes: Network didn't capture all peaks\n",
        "- Mode collapse: Samples cluster at single peak\n",
        "- Spread mismatch: Wrong variance indicates poor score magnitude learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Compare Generated Samples with Original Data\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "binsize = 30\n",
        "\n",
        "fig_samples = Figure(size=(900, 400))\n",
        "\n",
        "ax1 = Axis(fig_samples[1, 1], title=\"xâ‚ Marginal Distribution\", \n",
        "           xlabel=\"xâ‚\", ylabel=\"Density\")\n",
        "hist!(ax1, xâ‚€[1, :], bins=binsize, normalization=:pdf, \n",
        "      color=(:steelblue, 0.6), label=\"Generated\")\n",
        "hist!(ax1, data[1, :], bins=binsize, normalization=:pdf, \n",
        "      color=(:crimson, 0.4), label=\"Original\")\n",
        "axislegend(ax1, position=:rt)\n",
        "\n",
        "ax2 = Axis(fig_samples[1, 2], title=\"xâ‚‚ Marginal Distribution\", \n",
        "           xlabel=\"xâ‚‚\", ylabel=\"Density\")\n",
        "hist!(ax2, xâ‚€[2, :], bins=binsize, normalization=:pdf, \n",
        "      color=(:steelblue, 0.6), label=\"Generated\")\n",
        "hist!(ax2, data[2, :], bins=binsize, normalization=:pdf, \n",
        "      color=(:crimson, 0.4), label=\"Original\")\n",
        "axislegend(ax2, position=:rt)\n",
        "\n",
        "display(fig_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "julia"
        }
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Statistical Comparison\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "println(\"=\"^60)\n",
        "println(\"STATISTICAL COMPARISON: Generated vs Original\")\n",
        "println(\"=\"^60)\n",
        "println()\n",
        "println(\"Mean:\")\n",
        "println(\"  Generated: \", round.(vec(mean(xâ‚€, dims=2)), digits=4))\n",
        "println(\"  Original:  \", round.(vec(mean(data, dims=2)), digits=4))\n",
        "println()\n",
        "println(\"Covariance (Generated):\")\n",
        "cov_gen = cov(xâ‚€, dims=2)\n",
        "println(\"  [\", round(cov_gen[1,1], digits=4), \"  \", round(cov_gen[1,2], digits=4), \"]\")\n",
        "println(\"  [\", round(cov_gen[2,1], digits=4), \"  \", round(cov_gen[2,2], digits=4), \"]\")\n",
        "println()\n",
        "println(\"Covariance (Original):\")\n",
        "cov_orig = cov(data, dims=2)\n",
        "println(\"  [\", round(cov_orig[1,1], digits=4), \"  \", round(cov_orig[1,2], digits=4), \"]\")\n",
        "println(\"  [\", round(cov_orig[2,1], digits=4), \"  \", round(cov_orig[2,2], digits=4), \"]\")\n",
        "println(\"=\"^60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "### What We Learned\n",
        "\n",
        "1. **Score functions** $\\mathbf{s}(\\mathbf{x}) = \\nabla \\log p(\\mathbf{x})$ encode the gradient of log-probability\n",
        "\n",
        "2. **Denoising Score Matching** provides a practical training objective:\n",
        "   $$\\mathcal{L}_{\\text{DSM}} = \\mathbb{E}\\left[ \\left\\| \\mathbf{s}_\\theta(\\mathbf{x} + \\sigma\\mathbf{z}) + \\frac{\\mathbf{z}}{\\sigma} \\right\\|^2 \\right]$$\n",
        "\n",
        "3. **Neural networks** can approximate score functions effectively\n",
        "\n",
        "4. **Langevin dynamics** enables sampling using only the score function\n",
        "\n",
        "### Extensions\n",
        "\n",
        "This tutorial covers the basics. Modern score-based models extend these ideas:\n",
        "\n",
        "- **Multiple noise levels**: Train on a range of $\\sigma$ values (Song & Ermon, 2019)\n",
        "- **Continuous diffusion**: Use SDEs with time-varying noise (Song et al., 2021)\n",
        "- **Conditional generation**: Add conditioning variables for controlled generation\n",
        "- **Latent diffusion**: Work in compressed latent spaces (Rombach et al., 2022)\n",
        "\n",
        "### References\n",
        "\n",
        "- Vincent, P. (2011). \"A Connection Between Score Matching and Denoising Autoencoders\"\n",
        "- Song, Y., & Ermon, S. (2019). \"Generative Modeling by Estimating Gradients of the Data Distribution\"\n",
        "- Song, Y., et al. (2021). \"Score-Based Generative Modeling through Stochastic Differential Equations\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
