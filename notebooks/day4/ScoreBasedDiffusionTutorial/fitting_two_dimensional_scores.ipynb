{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score-Based Generative Modeling in 2D\n",
    "\n",
    "## Tutorial: Learning Score Functions with Denoising Score Matching\n",
    "\n",
    "This notebook demonstrates **score-based generative modeling** using Julia, a powerful approach for learning probability distributions and generating samples. We'll work with a 2D example to visualize the concepts clearly.\n",
    "\n",
    "### What You'll Learn\n",
    "1. What is a **score function** and why it's useful\n",
    "2. The **denoising score matching** objective\n",
    "3. How to train a neural network to approximate scores\n",
    "4. How to generate samples using **Langevin dynamics**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This tutorial requires the following Julia packages:\n",
    "\n",
    "| Package | Purpose |\n",
    "|---------|----------|\n",
    "| `GLMakie` | Interactive plotting and visualization |\n",
    "| `LinearAlgebra` | Matrix operations and linear algebra |\n",
    "| `ProgressBars` | Training progress visualization |\n",
    "| `Random` | Random number generation |\n",
    "| `Statistics` | Statistical functions (mean, cov, etc.) |\n",
    "| `HDF5` | Reading data from HDF5 files |\n",
    "| `Enzyme` | Automatic differentiation for gradients |\n",
    "\n",
    "Make sure to activate the project environment before running:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update! (generic function with 3 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using GLMakie, LinearAlgebra, ProgressBars, Random, Statistics, HDF5\n",
    "\n",
    "Random.seed!(1234)  # For reproducibility\n",
    "\n",
    "# Include custom neural network implementations\n",
    "include(\"simple_networks.jl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Score Functions\n",
    "\n",
    "### What is a Score Function?\n",
    "\n",
    "For a probability distribution $p(\\mathbf{x})$, the **score function** is defined as:\n",
    "\n",
    "$$\\mathbf{s}(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\log p(\\mathbf{x})$$\n",
    "\n",
    "The score tells us the direction of steepest increase in log-probability. In other words, it points toward regions of higher probability density.\n",
    "\n",
    "### Why Use Scores?\n",
    "\n",
    "1. **Avoids normalization**: Unlike $p(\\mathbf{x})$, the score doesn't require computing the partition function $Z = \\int p(\\mathbf{x}) d\\mathbf{x}$\n",
    "2. **Enables sampling**: We can generate samples using Langevin dynamics\n",
    "3. **Foundation for diffusion models**: Modern generative AI (DALL-E, Stable Diffusion) builds on these ideas\n",
    "\n",
    "### Our Target Distribution\n",
    "\n",
    "We'll work with a 2D distribution defined by the potential energy function:\n",
    "\n",
    "$$V(\\mathbf{x}) = \\frac{(x_1^2 - 1)^2}{4} + \\frac{(x_2^2 - 1)^2}{4} + \\frac{x_1 x_2}{3}$$\n",
    "\n",
    "The probability distribution is:\n",
    "$$p(\\mathbf{x}) \\propto \\exp(-V(\\mathbf{x}))$$\n",
    "\n",
    "This creates a **bimodal distribution** with two peaks (modes) due to the $(x^2 - 1)^2$ terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exact_score"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Score Functions (2D)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\"\"\"\n",
    "Exact score function from the potential V(x) = (x₁²-1)²/4 + (x₂²-1)²/4 + x₁x₂/3\n",
    "\n",
    "For p(x) ∝ exp(-V(x)), the score is:\n",
    "    s(x) = ∇log p(x) = -∇V(x)\n",
    "\n",
    "Computing ∇V:\n",
    "    ∂V/∂x₁ = x₁³ - x₁ + x₂/3\n",
    "    ∂V/∂x₂ = x₂³ - x₂ + x₁/3\n",
    "\n",
    "Therefore:\n",
    "    s(x) = [x₁ - x₁³ - x₂/3, x₂ - x₂³ - x₁/3]\n",
    "\"\"\"\n",
    "function exact_score(x)\n",
    "    x1, x2 = x[1], x[2]\n",
    "    return [x1 - x1^3 - x2/3, x2 - x2^3 - x1/3]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Score (Empirical)\n",
    "\n",
    "When we only have samples from the distribution (no analytical form), we can estimate the score using a **kernel density estimate** with Gaussian kernels:\n",
    "\n",
    "$$\\hat{p}(\\mathbf{x}) = \\frac{1}{M} \\sum_{i=1}^{M} \\mathcal{N}(\\mathbf{x} | \\mathbf{x}_i, \\sigma^2 I)$$\n",
    "\n",
    "The score of this mixture is:\n",
    "\n",
    "$$\\mathbf{s}_{\\text{GM}}(\\mathbf{x}) = \\frac{\\sum_{i=1}^{M} w_i (\\mathbf{x}_i - \\mathbf{x})}{\\sigma^2}$$\n",
    "\n",
    "where $w_i \\propto \\exp\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_i\\|^2}{2\\sigma^2}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gaussian_mixture_score"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gaussian mixture score estimator.\n",
    "\n",
    "This computes the score of a kernel density estimate formed by placing\n",
    "Gaussian kernels at each data point.\n",
    "\n",
    "Arguments:\n",
    "    x     : 2-element vector (query point)\n",
    "    data  : 2×M matrix (data samples)\n",
    "    sigma : kernel bandwidth\n",
    "\"\"\"\n",
    "function gaussian_mixture_score(x, data, sigma)\n",
    "    m = size(data, 2)\n",
    "    score_value = zeros(2)\n",
    "    denominator = 0.0\n",
    "    \n",
    "    for i in 1:m\n",
    "        Δ = data[:, i] .- x\n",
    "        U = exp(-(0.5 / sigma^2) * dot(Δ, Δ))\n",
    "        score_value .+= U .* Δ\n",
    "        denominator += U\n",
    "    end\n",
    "    \n",
    "    return score_value ./ (denominator * sigma^2)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Denoising Score Matching\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "We want to train a neural network $\\mathbf{s}_\\theta(\\mathbf{x})$ to approximate the true score $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$.\n",
    "\n",
    "**Naive approach (doesn't work):**\n",
    "$$\\mathcal{L}_{\\text{naive}} = \\mathbb{E}_{p(\\mathbf{x})} \\left[ \\| \\mathbf{s}_\\theta(\\mathbf{x}) - \\nabla_\\mathbf{x} \\log p(\\mathbf{x}) \\|^2 \\right]$$\n",
    "\n",
    "This requires knowing $\\nabla_\\mathbf{x} \\log p(\\mathbf{x})$ — but that's exactly what we're trying to learn!\n",
    "\n",
    "### The Denoising Score Matching Trick\n",
    "\n",
    "**Key insight**: Instead of matching the score of the data distribution, we match the score of a **noise-perturbed** distribution!\n",
    "\n",
    "Given a data point $\\mathbf{x}$, we create a noisy version:\n",
    "$$\\tilde{\\mathbf{x}} = \\mathbf{x} + \\sigma \\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(0, I)$$\n",
    "\n",
    "The score of the conditional distribution $p(\\tilde{\\mathbf{x}} | \\mathbf{x})$ is:\n",
    "$$\\nabla_{\\tilde{\\mathbf{x}}} \\log p(\\tilde{\\mathbf{x}} | \\mathbf{x}) = -\\frac{\\tilde{\\mathbf{x}} - \\mathbf{x}}{\\sigma^2} = -\\frac{\\mathbf{z}}{\\sigma}$$\n",
    "\n",
    "### Denoising Score Matching Loss\n",
    "\n",
    "The **denoising score matching** objective is:\n",
    "\n",
    "$$\\boxed{\\mathcal{L}_{\\text{DSM}} = \\mathbb{E}_{\\mathbf{x} \\sim p(\\mathbf{x})} \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(0, I)} \\left[ \\left\\| \\mathbf{s}_\\theta(\\mathbf{x} + \\sigma\\mathbf{z}) + \\frac{\\mathbf{z}}{\\sigma} \\right\\|^2 \\right]}$$\n",
    "\n",
    "**Why this works:**\n",
    "- We know $\\mathbf{z}$ (we sampled it!)\n",
    "- The network learns to predict $-\\mathbf{z}/\\sigma$, which is the score of the noisy distribution\n",
    "- As $\\sigma \\to 0$, this converges to the true data score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denoising_loss_function (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Loss Functions (2D)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\"\"\"\n",
    "Gaussian mixture loss function.\n",
    "\n",
    "Trains the network to match the Gaussian mixture score estimate.\n",
    "This is more expensive but provides a ground truth to compare against.\n",
    "\"\"\"\n",
    "function gaussian_mixture_loss_function(model, data, σ, zs)\n",
    "    batchsize = size(data, 2)\n",
    "    lossval = [0.0]\n",
    "    \n",
    "    for i in 1:batchsize\n",
    "        x = data[:, i] .+ σ .* zs[:, i]\n",
    "        ŷ = model(x)\n",
    "        y = gaussian_mixture_score(x, data, σ)\n",
    "        lossval[1] += sum((y .- ŷ).^2) / batchsize\n",
    "    end\n",
    "    \n",
    "    return lossval[1]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Denoising Score Matching loss function.\n",
    "\n",
    "This is the KEY loss function:\n",
    "    L = E[(s_θ(x̃) + z/σ)²]\n",
    "    \n",
    "where x̃ = x + σz and z ~ N(0, I)\n",
    "\n",
    "The network learns to predict -z/σ, which equals the score of the\n",
    "noise-perturbed distribution.\n",
    "\"\"\"\n",
    "function denoising_loss_function(model, data, sigma, noises)\n",
    "    batchsize = size(data, 2)\n",
    "    lossval = [0.0]\n",
    "    \n",
    "    for i in 1:batchsize\n",
    "        x = data[:, i]           # Original data point\n",
    "        z = noises[:, i]         # Random noise\n",
    "        x̃ = x .+ sigma .* z     # Perturbed point\n",
    "        ŷ = model(x̃)            # Network prediction\n",
    "        \n",
    "        # Loss: (prediction + z/σ)²\n",
    "        # Network should predict -z/σ\n",
    "        lossval[1] += sum((ŷ .+ z ./ sigma).^2) / batchsize\n",
    "    end\n",
    "    \n",
    "    return lossval[1]\n",
    "end\n",
    "\n",
    "# Use denoising score matching for training\n",
    "loss_function = denoising_loss_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Loading and Visualizing the Data\n",
    "\n",
    "We load samples from our target distribution, generated using Langevin dynamics on the potential $V(\\mathbf{x})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 samples of 2D data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Load Data and Setup\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Load 2D data from HDF5 file\n",
    "data = h5read(\"potential_data_2D.hdf5\", \"data\")  # 2×M matrix\n",
    "M = size(data, 2)\n",
    "println(\"Loaded $(M) samples of 2D data\")\n",
    "\n",
    "# Noise level for score matching\n",
    "sigma = σ = 0.05\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 1: Data Distribution\n",
    "\n",
    "This figure shows the **marginal distributions** of our 2D data along each axis.\n",
    "\n",
    "**What to observe:**\n",
    "- **Bimodal structure**: Each dimension shows two peaks around $x \\approx \\pm 1$\n",
    "- This comes from the $(x^2 - 1)^2$ terms in the potential, which have minima at $x = \\pm 1$\n",
    "- The coupling term $x_1 x_2 / 3$ creates correlation between the dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLMakie.Screen(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize data distribution\n",
    "fig = Figure(size=(800, 400))\n",
    "ax1 = Axis(fig[1, 1], title=\"x₁ Distribution\", xlabel=\"x₁\", ylabel=\"Density\")\n",
    "hist!(ax1, data[1, :], bins=100, normalization=:pdf, color=:steelblue)\n",
    "ax2 = Axis(fig[1, 2], title=\"x₂ Distribution\", xlabel=\"x₂\", ylabel=\"Density\")\n",
    "hist!(ax2, data[2, :], bins=100, normalization=:pdf, color=:steelblue)\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "# Test score computation on a grid\n",
    "test_point = [0.0, 0.0]\n",
    "tmp = gaussian_mixture_score(test_point, data, sigma)\n",
    "println(\"Gaussian mixture score at origin: \", tmp)\n",
    "println(\"Exact score at origin: \", exact_score(test_point))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Neural Network Architecture\n",
    "\n",
    "We use a simple feedforward neural network with:\n",
    "- **Input**: 2D position $\\mathbf{x}$\n",
    "- **Hidden layer**: 50 neurons with Swish activation\n",
    "- **Linear bypass**: A direct linear connection from input to output\n",
    "- **Output**: 2D score estimate $\\mathbf{s}_\\theta(\\mathbf{x})$\n",
    "\n",
    "The **linear bypass** helps the network learn linear score components directly, which is important since the score has both linear and nonlinear terms:\n",
    "\n",
    "$$\\mathbf{s}(\\mathbf{x}) = \\begin{bmatrix} x_1 - x_1^3 - x_2/3 \\\\ x_2 - x_2^3 - x_1/3 \\end{bmatrix} = \\underbrace{\\begin{bmatrix} x_1 - x_2/3 \\\\ x_2 - x_1/3 \\end{bmatrix}}_{\\text{linear}} + \\underbrace{\\begin{bmatrix} -x_1^3 \\\\ -x_2^3 \\end{bmatrix}}_{\\text{nonlinear}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network architecture: 2 → 50 → 2 with linear bypass\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Define Network\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "Nθ = 2        # Input dimension (2D position)\n",
    "Nθᴴ = 50      # Hidden dimension\n",
    "Nout = 2      # Output dimension (2D score)\n",
    "\n",
    "# Create network with linear bypass for better learning\n",
    "network = OneLayerNetworkWithLinearByPass(Nθ, Nout, Nθᴴ)\n",
    "dnetwork = deepcopy(network)           # For storing gradients\n",
    "smoothed_network = deepcopy(network)   # For exponential moving average\n",
    "\n",
    "println(\"Network architecture: $(Nθ) → $(Nθᴴ) → $(Nout) with linear bypass\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss on test batch: 724.529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "exactscore (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test loss computation\n",
    "test_data = data[:, 1:10]\n",
    "test_noise = randn(2, 10)\n",
    "initial_loss = loss_function(network, test_data, σ, test_noise)\n",
    "println(\"Initial loss on test batch: $(round(initial_loss, digits=3))\")\n",
    "\n",
    "# Create convenience functions for score computation\n",
    "gmscore(x) = gaussian_mixture_score(x, data, sigma)\n",
    "exactscore(x) = exact_score(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training the Network\n",
    "\n",
    "We train using:\n",
    "- **Adam optimizer**: Adaptive learning rates for each parameter\n",
    "- **Mini-batch gradient descent**: Process data in batches of 20 samples\n",
    "- **Enzyme.jl**: Automatic differentiation for computing gradients\n",
    "- **Exponential moving average**: Smooth network parameters for stability\n",
    "\n",
    "### Training Loop Details\n",
    "\n",
    "For each epoch:\n",
    "1. Shuffle training data into mini-batches\n",
    "2. For each batch:\n",
    "   - Sample fresh noise $\\mathbf{z}$\n",
    "   - Compute denoising loss\n",
    "   - Backpropagate and update weights\n",
    "3. Evaluate loss on held-out test data\n",
    "4. Update exponential moving average of weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 200 epochs...\n",
      "Batch size: 20, Total samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                             ┫ 0/200 [00:01<-3:-37, -1s/it]\n",
      "0.5%┣▏                                         ┫ 1/200 [00:08<Inf:Inf, InfGs/it]\n",
      "1.5%┣▊                                              ┫ 3/200 [00:08<13:56, 4s/it]\n",
      "2.5%┣█▏                                             ┫ 5/200 [00:09<06:58, 2s/it]\n",
      "3.5%┣█▋                                             ┫ 7/200 [00:09<04:38, 1s/it]\n",
      "4.5%┣██▏                                            ┫ 9/200 [00:09<03:28, 1s/it]\n",
      "5.5%┣██▌                                           ┫ 11/200 [00:09<02:46, 1it/s]\n",
      "6.5%┣███                                           ┫ 13/200 [00:09<02:18, 1it/s]\n",
      "7.5%┣███▌                                          ┫ 15/200 [00:09<01:58, 2it/s]\n",
      "8.5%┣████                                          ┫ 17/200 [00:09<01:43, 2it/s]\n",
      "9.5%┣████▍                                         ┫ 19/200 [00:09<01:31, 2it/s]\n",
      "10.5%┣████▊                                        ┫ 21/200 [00:09<01:22, 2it/s]\n",
      "11.5%┣█████▏                                       ┫ 23/200 [00:09<01:14, 2it/s]\n",
      "12.5%┣█████▋                                       ┫ 25/200 [00:09<01:08, 3it/s]\n",
      "13.5%┣██████                                       ┫ 27/200 [00:09<01:02, 3it/s]\n",
      "14.5%┣██████▌                                      ┫ 29/200 [00:09<00:58, 3it/s]\n",
      "15.5%┣███████                                      ┫ 31/200 [00:10<00:54, 3it/s]\n",
      "16.5%┣███████▍                                     ┫ 33/200 [00:10<00:50, 3it/s]\n",
      "17.5%┣███████▉                                     ┫ 35/200 [00:10<00:47, 4it/s]\n",
      "18.5%┣████████▎                                    ┫ 37/200 [00:10<00:44, 4it/s]\n",
      "19.5%┣████████▊                                    ┫ 39/200 [00:10<00:42, 4it/s]\n",
      "20.5%┣█████████▎                                   ┫ 41/200 [00:10<00:39, 4it/s]\n",
      "21.5%┣█████████▊                                   ┫ 43/200 [00:10<00:37, 4it/s]\n",
      "22.5%┣██████████▏                                  ┫ 45/200 [00:10<00:35, 4it/s]\n",
      "23.5%┣██████████▋                                  ┫ 47/200 [00:10<00:34, 5it/s]\n",
      "24.5%┣███████████                                  ┫ 49/200 [00:10<00:32, 5it/s]\n",
      "25.5%┣███████████▌                                 ┫ 51/200 [00:10<00:31, 5it/s]\n",
      "26.5%┣████████████                                 ┫ 53/200 [00:10<00:29, 5it/s]\n",
      "27.5%┣████████████▍                                ┫ 55/200 [00:10<00:28, 5it/s]\n",
      "28.5%┣████████████▉                                ┫ 57/200 [00:10<00:27, 5it/s]\n",
      "29.5%┣█████████████▎                               ┫ 59/200 [00:11<00:26, 5it/s]\n",
      "30.5%┣█████████████▊                               ┫ 61/200 [00:11<00:25, 6it/s]\n",
      "31.5%┣██████████████▏                              ┫ 63/200 [00:11<00:24, 6it/s]\n",
      "32.5%┣██████████████▋                              ┫ 65/200 [00:11<00:23, 6it/s]\n",
      "33.5%┣███████████████                              ┫ 67/200 [00:11<00:22, 6it/s]\n",
      "34.5%┣███████████████▌                             ┫ 69/200 [00:11<00:21, 6it/s]\n",
      "35.5%┣████████████████                             ┫ 71/200 [00:11<00:20, 6it/s]\n",
      "36.5%┣████████████████▍                            ┫ 73/200 [00:11<00:20, 7it/s]\n",
      "37.5%┣████████████████▉                            ┫ 75/200 [00:11<00:19, 7it/s]\n",
      "38.5%┣█████████████████▎                           ┫ 77/200 [00:11<00:18, 7it/s]\n",
      "39.5%┣█████████████████▊                           ┫ 79/200 [00:11<00:18, 7it/s]\n",
      "40.5%┣██████████████████▎                          ┫ 81/200 [00:11<00:17, 7it/s]\n",
      "41.5%┣██████████████████▊                          ┫ 83/200 [00:11<00:16, 7it/s]\n",
      "42.5%┣███████████████████▏                         ┫ 85/200 [00:12<00:16, 7it/s]\n",
      "43.5%┣███████████████████▋                         ┫ 87/200 [00:12<00:15, 7it/s]\n",
      "44.5%┣████████████████████                         ┫ 89/200 [00:12<00:15, 8it/s]\n",
      "45.5%┣████████████████████▌                        ┫ 91/200 [00:12<00:14, 8it/s]\n",
      "46.5%┣█████████████████████                        ┫ 93/200 [00:12<00:14, 8it/s]\n",
      "47.5%┣█████████████████████▍                       ┫ 95/200 [00:12<00:13, 8it/s]\n",
      "48.5%┣█████████████████████▉                       ┫ 97/200 [00:12<00:13, 8it/s]\n",
      "49.5%┣██████████████████████▎                      ┫ 99/200 [00:12<00:12, 8it/s]\n",
      "50.5%┣██████████████████████▏                     ┫ 101/200 [00:12<00:12, 8it/s]\n",
      "51.5%┣██████████████████████▋                     ┫ 103/200 [00:12<00:12, 8it/s]\n",
      "52.5%┣███████████████████████                     ┫ 105/200 [00:12<00:11, 8it/s]\n",
      "53.5%┣███████████████████████▌                    ┫ 107/200 [00:12<00:11, 9it/s]\n",
      "54.5%┣████████████████████████                    ┫ 109/200 [00:12<00:10, 9it/s]\n",
      "55.5%┣████████████████████████▍                   ┫ 111/200 [00:12<00:10, 9it/s]\n",
      "56.5%┣████████████████████████▉                   ┫ 113/200 [00:13<00:10, 9it/s]\n",
      "57.5%┣█████████████████████████▎                  ┫ 115/200 [00:13<00:09, 9it/s]\n",
      "58.5%┣█████████████████████████▊                  ┫ 117/200 [00:13<00:09, 9it/s]\n",
      "59.5%┣██████████████████████████▏                 ┫ 119/200 [00:13<00:09, 9it/s]\n",
      "60.5%┣██████████████████████████▋                 ┫ 121/200 [00:13<00:08, 9it/s]\n",
      "61.5%┣███████████████████████████                 ┫ 123/200 [00:13<00:08, 9it/s]\n",
      "62.5%┣██████████████████████████▉                ┫ 125/200 [00:13<00:08, 10it/s]\n",
      "63.5%┣███████████████████████████▎               ┫ 127/200 [00:13<00:08, 10it/s]\n",
      "64.5%┣███████████████████████████▊               ┫ 129/200 [00:13<00:07, 10it/s]\n",
      "65.5%┣████████████████████████████▏              ┫ 131/200 [00:13<00:07, 10it/s]\n",
      "66.5%┣████████████████████████████▋              ┫ 133/200 [00:13<00:07, 10it/s]\n",
      "67.5%┣█████████████████████████████              ┫ 135/200 [00:13<00:06, 10it/s]\n",
      "68.5%┣█████████████████████████████▌             ┫ 137/200 [00:13<00:06, 10it/s]\n",
      "69.5%┣█████████████████████████████▉             ┫ 139/200 [00:14<00:06, 10it/s]\n",
      "70.5%┣██████████████████████████████▎            ┫ 141/200 [00:14<00:06, 10it/s]\n",
      "71.5%┣██████████████████████████████▊            ┫ 143/200 [00:14<00:05, 10it/s]\n",
      "72.5%┣███████████████████████████████▏           ┫ 145/200 [00:14<00:05, 10it/s]\n",
      "73.5%┣███████████████████████████████▋           ┫ 147/200 [00:14<00:05, 11it/s]\n",
      "74.5%┣████████████████████████████████           ┫ 149/200 [00:14<00:05, 11it/s]\n",
      "75.5%┣████████████████████████████████▌          ┫ 151/200 [00:14<00:05, 11it/s]\n",
      "76.5%┣█████████████████████████████████          ┫ 153/200 [00:14<00:04, 11it/s]\n",
      "77.5%┣█████████████████████████████████▎         ┫ 155/200 [00:14<00:04, 11it/s]\n",
      "78.5%┣█████████████████████████████████▊         ┫ 157/200 [00:14<00:04, 11it/s]\n",
      "79.5%┣██████████████████████████████████▏        ┫ 159/200 [00:14<00:04, 11it/s]\n",
      "80.5%┣██████████████████████████████████▋        ┫ 161/200 [00:14<00:03, 11it/s]\n",
      "81.5%┣███████████████████████████████████        ┫ 163/200 [00:14<00:03, 11it/s]\n",
      "82.5%┣███████████████████████████████████▌       ┫ 165/200 [00:14<00:03, 11it/s]\n",
      "83.5%┣████████████████████████████████████       ┫ 167/200 [00:15<00:03, 11it/s]\n",
      "84.5%┣████████████████████████████████████▍      ┫ 169/200 [00:15<00:03, 11it/s]\n",
      "85.5%┣████████████████████████████████████▊      ┫ 171/200 [00:15<00:03, 12it/s]\n",
      "86.5%┣█████████████████████████████████████▏     ┫ 173/200 [00:15<00:02, 12it/s]\n",
      "87.5%┣█████████████████████████████████████▋     ┫ 175/200 [00:15<00:02, 12it/s]\n",
      "88.5%┣██████████████████████████████████████     ┫ 177/200 [00:15<00:02, 12it/s]\n",
      "89.5%┣██████████████████████████████████████▌    ┫ 179/200 [00:15<00:02, 12it/s]\n",
      "90.5%┣███████████████████████████████████████    ┫ 181/200 [00:15<00:02, 12it/s]\n",
      "91.5%┣███████████████████████████████████████▍   ┫ 183/200 [00:15<00:01, 12it/s]\n",
      "92.5%┣███████████████████████████████████████▊   ┫ 185/200 [00:15<00:01, 12it/s]\n",
      "93.5%┣████████████████████████████████████████▏  ┫ 187/200 [00:15<00:01, 12it/s]\n",
      "94.5%┣████████████████████████████████████████▋  ┫ 189/200 [00:15<00:01, 12it/s]\n",
      "95.5%┣█████████████████████████████████████████  ┫ 191/200 [00:15<00:01, 12it/s]\n",
      "96.5%┣█████████████████████████████████████████▌ ┫ 193/200 [00:16<00:01, 12it/s]\n",
      "97.5%┣██████████████████████████████████████████ ┫ 195/200 [00:16<00:00, 12it/s]\n",
      "98.5%┣██████████████████████████████████████████▍┫ 197/200 [00:16<00:00, 12it/s]\n",
      "99.5%┣██████████████████████████████████████████▉┫ 199/200 [00:16<00:00, 13it/s]\n",
      "\u001b[1A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Final training loss: 793.341\n",
      "Final test loss: 811.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%┣██████████████████████████████████████████┫ 200/200 [00:16<00:00, 13it/s]\n",
      "100.0%┣██████████████████████████████████████████┫ 200/200 [00:16<00:00, 13it/s]\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Training\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "adam = Adam(network)          # Initialize Adam optimizer\n",
    "batchsize = 20                # Samples per batch\n",
    "loss_list = Float64[]         # Track training loss\n",
    "test_loss_list = Float64[]    # Track test loss\n",
    "epochs = 200                  # Number of training epochs\n",
    "network_parameters = copy(parameters(network))  # For EMA smoothing\n",
    "\n",
    "println(\"Starting training for $(epochs) epochs...\")\n",
    "println(\"Batch size: $(batchsize), Total samples: $(M)\")\n",
    "\n",
    "for i in ProgressBar(1:epochs)\n",
    "    # Split data: odd indices for training, even for testing\n",
    "    shuffled_list = chunk_list(shuffle(1:2:M), batchsize)\n",
    "    shuffled_test_list = chunk_list(shuffle(2:2:M), batchsize)\n",
    "    loss_value = 0.0\n",
    "    N = length(shuffled_list)\n",
    "    \n",
    "    # ─── Training Pass ───\n",
    "    for permuted_list in shuffled_list\n",
    "        θbatch = data[:, permuted_list]  # 2×batchsize matrix\n",
    "        zero!(dnetwork)                   # Reset gradients\n",
    "        zs = randn(2, length(permuted_list))  # Sample fresh noise\n",
    "        \n",
    "        # Compute gradients via Enzyme autodiff\n",
    "        autodiff(Enzyme.Reverse, loss_function, Active, \n",
    "                 DuplicatedNoNeed(network, dnetwork), \n",
    "                 Const(θbatch), Const(sigma), Const(zs))\n",
    "        \n",
    "        # Update network with Adam\n",
    "        update!(adam, network, dnetwork)\n",
    "        loss_value += loss_function(network, θbatch, sigma, zs) / N\n",
    "    end\n",
    "    push!(loss_list, loss_value)\n",
    "    \n",
    "    # ─── Test Pass ───\n",
    "    loss_value = 0.0\n",
    "    N = length(shuffled_test_list)\n",
    "    for permuted_list in shuffled_test_list\n",
    "        θbatch = data[:, permuted_list]\n",
    "        zs = randn(2, length(permuted_list))\n",
    "        loss_value += loss_function(network, θbatch, sigma, zs) / N\n",
    "    end\n",
    "    push!(test_loss_list, loss_value)\n",
    "    \n",
    "    # ─── Exponential Moving Average ───\n",
    "    m = 0.9  # Smoothing factor\n",
    "    network_parameters .= m * network_parameters + (1 - m) * parameters(network)\n",
    "    set_parameters!(smoothed_network, network_parameters)\n",
    "end\n",
    "\n",
    "println(\"\\nTraining complete!\")\n",
    "println(\"Final training loss: $(round(loss_list[end], digits=4))\")\n",
    "println(\"Final test loss: $(round(test_loss_list[end], digits=4))\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: Training Loss Curves\n",
    "\n",
    "This figure shows the **learning dynamics** during training.\n",
    "\n",
    "**What to observe:**\n",
    "- **Log-scale y-axis**: Helps visualize orders of magnitude improvement\n",
    "- **Training loss (blue)**: Should decrease steadily\n",
    "- **Test loss (red)**: Should follow training loss closely\n",
    "- **Convergence**: Both losses stabilize after sufficient epochs\n",
    "- **No overfitting**: If test loss diverges from training loss, the model is overfitting\n",
    "\n",
    "**Note**: The denoising loss has an irreducible minimum — even the perfect score function has nonzero loss due to the stochastic noise $\\mathbf{z}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLMakie.Screen(...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Plot Training Loss\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "loss_fig = Figure(size=(700, 400))\n",
    "ax = Axis(loss_fig[1, 1]; \n",
    "          title=\"Training Progress\", \n",
    "          xlabel=\"Epoch\", \n",
    "          ylabel=\"Log₁₀(Loss)\")\n",
    "scatter!(ax, log10.(loss_list); color=:steelblue, markersize=6, label=\"Training Loss\")\n",
    "scatter!(ax, log10.(test_loss_list); color=:crimson, markersize=6, label=\"Test Loss\")\n",
    "axislegend(ax, position=:rt)\n",
    "display(loss_fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Evaluating the Learned Score\n",
    "\n",
    "We compare three score functions:\n",
    "1. **Network**: Our trained neural network $\\mathbf{s}_\\theta$\n",
    "2. **Gaussian Mixture**: The kernel density estimate score\n",
    "3. **Exact**: The analytical score from $-\\nabla V$\n",
    "\n",
    "Lower loss indicates better score estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating score functions (averaging over 10 noise samples)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                               ┫ 0/10 [00:00<00:00, -0s/it]\n",
      "10.0%┣████▏                                     ┫ 1/10 [00:00<Inf:Inf, InfGs/it]\n",
      "20.0%┣█████████▍                                     ┫ 2/10 [00:00<00:04, 2it/s]\n",
      "30.0%┣██████████████                                 ┫ 3/10 [00:01<00:02, 3it/s]\n",
      "40.0%┣██████████████████▉                            ┫ 4/10 [00:01<00:02, 4it/s]\n",
      "50.0%┣███████████████████████▌                       ┫ 5/10 [00:01<00:01, 4it/s]\n",
      "60.0%┣████████████████████████████▏                  ┫ 6/10 [00:01<00:01, 5it/s]\n",
      "70.0%┣█████████████████████████████████              ┫ 7/10 [00:01<00:01, 5it/s]\n",
      "80.0%┣█████████████████████████████████████▋         ┫ 8/10 [00:01<00:00, 5it/s]\n",
      "90.0%┣██████████████████████████████████████████▎    ┫ 9/10 [00:02<00:00, 5it/s]\n",
      "\u001b[1A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SCORE FUNCTION COMPARISON\n",
      "==================================================\n",
      "Network Loss:          796.06\n",
      "Gaussian Mixture Loss: 756.97\n",
      "Exact Score Loss:      810.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%┣█████████████████████████████████████████████┫ 10/10 [00:02<00:00, 5it/s]\n",
      "100.0%┣█████████████████████████████████████████████┫ 10/10 [00:02<00:00, 5it/s]\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Evaluate Final Losses\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "\"\"\"\n",
    "Evaluate denoising loss for any score function.\n",
    "This allows us to compare different score estimators on the same footing.\n",
    "\"\"\"\n",
    "function denoising_loss_with_fn(score_fn, data, σ, noises)\n",
    "    batchsize = size(data, 2)\n",
    "    lossval = 0.0\n",
    "    \n",
    "    for i in 1:batchsize\n",
    "        x = data[:, i]\n",
    "        z = noises[:, i]\n",
    "        x̃ = x .+ σ .* z\n",
    "        ŷ = score_fn(x̃)\n",
    "        lossval += sum((ŷ .+ z ./ σ).^2) / batchsize\n",
    "    end\n",
    "    \n",
    "    return lossval\n",
    "end\n",
    "\n",
    "# Define score functions for evaluation\n",
    "network_score_fn(x) = predict(network, x)\n",
    "\n",
    "# Average over multiple noise realizations for stable estimates\n",
    "skip = 50\n",
    "number_of_samples = 10\n",
    "\n",
    "println(\"Evaluating score functions (averaging over $(number_of_samples) noise samples)...\")\n",
    "\n",
    "l1 = mean([denoising_loss_with_fn(network_score_fn, data[:, 1:skip:end], σ, randn(2, M)) \n",
    "           for _ in 1:number_of_samples])\n",
    "l2 = mean([denoising_loss_with_fn(gmscore, data[:, 1:skip:end], σ, randn(2, M)) \n",
    "           for _ in ProgressBar(1:number_of_samples)])\n",
    "l3 = mean([denoising_loss_with_fn(exactscore, data[:, 1:skip:end], σ, randn(2, M)) \n",
    "           for _ in 1:number_of_samples])\n",
    "\n",
    "println(\"\\n\" * \"=\"^50)\n",
    "println(\"SCORE FUNCTION COMPARISON\")\n",
    "println(\"=\"^50)\n",
    "println(\"Network Loss:          $(round(l1, digits=2))\")\n",
    "println(\"Gaussian Mixture Loss: $(round(l2, digits=2))\")\n",
    "println(\"Exact Score Loss:      $(round(l3, digits=2))\")\n",
    "println(\"=\"^50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Visualizing the Learned Score Field\n",
    "\n",
    "### Figure 3: Score Vector Fields\n",
    "\n",
    "This figure compares the **score vector fields** from all three methods.\n",
    "\n",
    "**How to read these plots:**\n",
    "- **Arrows**: Point in the direction of increasing probability\n",
    "- **Arrow direction**: The score direction $\\mathbf{s}(\\mathbf{x}) / \\|\\mathbf{s}(\\mathbf{x})\\|$\n",
    "- **Gray dots**: Subsampled data points showing the true distribution\n",
    "\n",
    "**What to observe:**\n",
    "- All three methods should show arrows pointing **toward the modes** (high-density regions)\n",
    "- Near the origin, arrows point outward (low probability region between modes)\n",
    "- Near $(\\pm 1, \\pm 1)$, arrows point inward (high probability modes)\n",
    "- The network should approximate the exact score pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing score fields on grid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                               ┫ 0/30 [00:00<00:00, -0s/it]\n",
      "6.7%┣███▏                                           ┫ 2/30 [00:00<00:02, 16it/s]\n",
      "13.3%┣██████▏                                       ┫ 4/30 [00:00<00:01, 26it/s]\n",
      "20.0%┣█████████▏                                    ┫ 6/30 [00:00<00:01, 28it/s]\n",
      "30.0%┣█████████████▉                                ┫ 9/30 [00:00<00:01, 32it/s]\n",
      "40.0%┣██████████████████                           ┫ 12/30 [00:00<00:01, 35it/s]\n",
      "46.7%┣█████████████████████                        ┫ 14/30 [00:00<00:00, 36it/s]\n",
      "53.3%┣████████████████████████                     ┫ 16/30 [00:00<00:00, 36it/s]\n",
      "63.3%┣████████████████████████████▌                ┫ 19/30 [00:00<00:00, 38it/s]\n",
      "73.3%┣█████████████████████████████████            ┫ 22/30 [00:01<00:00, 38it/s]\n",
      "80.0%┣████████████████████████████████████         ┫ 24/30 [00:01<00:00, 38it/s]\n",
      "86.7%┣███████████████████████████████████████      ┫ 26/30 [00:01<00:00, 38it/s]\n",
      "96.7%┣███████████████████████████████████████████▌ ┫ 29/30 [00:01<00:00, 39it/s]\n",
      "100.0%┣████████████████████████████████████████████┫ 30/30 [00:01<00:00, 39it/s]\n",
      "100.0%┣████████████████████████████████████████████┫ 30/30 [00:01<00:00, 39it/s]\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Visualize Learned Score Field\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Create grid for visualization\n",
    "x_range = range(-2, 2, length=30)\n",
    "y_range = range(-2, 2, length=30)\n",
    "\n",
    "# Preallocate score arrays\n",
    "network_scores_u = zeros(length(x_range), length(y_range))\n",
    "network_scores_v = zeros(length(x_range), length(y_range))\n",
    "gm_scores_u = zeros(length(x_range), length(y_range))\n",
    "gm_scores_v = zeros(length(x_range), length(y_range))\n",
    "exact_scores_u = zeros(length(x_range), length(y_range))\n",
    "exact_scores_v = zeros(length(x_range), length(y_range))\n",
    "\n",
    "println(\"Computing score fields on grid...\")\n",
    "for (i, x) in ProgressBar(enumerate(x_range))\n",
    "    for (j, y) in enumerate(y_range)\n",
    "        # Network score\n",
    "        net_score = predict(network, [x, y])\n",
    "        network_scores_u[i, j] = net_score[1]\n",
    "        network_scores_v[i, j] = net_score[2]\n",
    "        \n",
    "        # Gaussian mixture score\n",
    "        gm_score = gaussian_mixture_score([x, y], data, sigma)\n",
    "        gm_scores_u[i, j] = gm_score[1]\n",
    "        gm_scores_v[i, j] = gm_score[2]\n",
    "        \n",
    "        # Exact score\n",
    "        exact_score_val = exact_score([x, y])\n",
    "        exact_scores_u[i, j] = exact_score_val[1]\n",
    "        exact_scores_v[i, j] = exact_score_val[2]\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30×30 Matrix{Float64}:\n",
       " -2.0  -1.86207  -1.72414  -1.58621  …  1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621  …  1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621  …  1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       "  ⋮                                  ⋱                             \n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621  …  1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621  …  1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0\n",
       " -2.0  -1.86207  -1.72414  -1.58621     1.58621  1.72414  1.86207  2.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize vectors for uniform arrow lengths (shows direction only)\n",
    "function normalize_vectors(u, v, scale=0.15)\n",
    "    mag = sqrt.(u.^2 .+ v.^2)\n",
    "    mag[mag .< 1e-10] .= 1e-10  # Avoid division by zero\n",
    "    return u .* scale ./ mag, v .* scale ./ mag\n",
    "end\n",
    "\n",
    "net_u_scaled, net_v_scaled = normalize_vectors(network_scores_u, network_scores_v)\n",
    "gm_u_scaled, gm_v_scaled = normalize_vectors(gm_scores_u, gm_scores_v)\n",
    "exact_u_scaled, exact_v_scaled = normalize_vectors(exact_scores_u, exact_scores_v)\n",
    "\n",
    "# Create grid coordinate matrices\n",
    "x_grid = [x for x in x_range, y in y_range]\n",
    "y_grid = [y for x in x_range, y in y_range]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`arrows` are deprecated in favor of `arrows2d` and `arrows3d`.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Makie ~/.julia/packages/Makie/Vn16E/src/basic_recipes/arrows.jl:166\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39marrowsize has been deprecated in favor of tipwidth and tiplength.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Makie ~/.julia/packages/Makie/Vn16E/src/basic_recipes/arrows.jl:206\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`arrows` are deprecated in favor of `arrows2d` and `arrows3d`.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Makie ~/.julia/packages/Makie/Vn16E/src/basic_recipes/arrows.jl:166\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39marrowsize has been deprecated in favor of tipwidth and tiplength.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Makie ~/.julia/packages/Makie/Vn16E/src/basic_recipes/arrows.jl:206\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39m`arrows` are deprecated in favor of `arrows2d` and `arrows3d`.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Makie ~/.julia/packages/Makie/Vn16E/src/basic_recipes/arrows.jl:166\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39marrowsize has been deprecated in favor of tipwidth and tiplength.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Makie ~/.julia/packages/Makie/Vn16E/src/basic_recipes/arrows.jl:206\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GLMakie.Screen(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot score vector field comparison\n",
    "fig = Figure(size=(1800, 600))\n",
    "\n",
    "# Network score field\n",
    "ax1 = Axis(fig[1, 1], \n",
    "           title=\"Network Score Field\\n(Loss: $(round(l1, digits=1)))\", \n",
    "           xlabel=\"x₁\", ylabel=\"x₂\", \n",
    "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
    "scatter!(ax1, data[1, 1:100:end], data[2, 1:100:end], \n",
    "         markersize=3, alpha=0.3, color=:gray)\n",
    "arrows!(ax1, vec(x_grid), vec(y_grid), vec(net_u_scaled), vec(net_v_scaled), \n",
    "        color=:steelblue, linewidth=1.5, arrowsize=8)\n",
    "\n",
    "# Gaussian mixture score field\n",
    "ax2 = Axis(fig[1, 2], \n",
    "           title=\"Gaussian Mixture Score Field\\n(Loss: $(round(l2, digits=1)))\", \n",
    "           xlabel=\"x₁\", ylabel=\"x₂\", \n",
    "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
    "scatter!(ax2, data[1, 1:100:end], data[2, 1:100:end], \n",
    "         markersize=3, alpha=0.3, color=:gray)\n",
    "arrows!(ax2, vec(x_grid), vec(y_grid), vec(gm_u_scaled), vec(gm_v_scaled), \n",
    "        color=:crimson, linewidth=1.5, arrowsize=8)\n",
    "\n",
    "# Exact score field\n",
    "ax3 = Axis(fig[1, 3], \n",
    "           title=\"Exact Score Field\\n(Loss: $(round(l3, digits=1)))\", \n",
    "           xlabel=\"x₁\", ylabel=\"x₂\", \n",
    "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
    "scatter!(ax3, data[1, 1:100:end], data[2, 1:100:end], \n",
    "         markersize=3, alpha=0.3, color=:gray)\n",
    "arrows!(ax3, vec(x_grid), vec(y_grid), vec(exact_u_scaled), vec(exact_v_scaled), \n",
    "        color=:forestgreen, linewidth=1.5, arrowsize=8)\n",
    "\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4: Score Magnitude Heatmaps\n",
    "\n",
    "This figure shows the **magnitude** of the score at each point: $\\|\\mathbf{s}(\\mathbf{x})\\|$.\n",
    "\n",
    "**What to observe:**\n",
    "- **Low magnitude** (dark) near modes: At high probability regions, the score is small\n",
    "- **High magnitude** (bright) away from modes: The score pushes samples toward high-density regions\n",
    "- **Saddle point at origin**: The score has a critical point at $(0, 0)$\n",
    "- **Symmetry**: The pattern should be symmetric due to the symmetric potential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLMakie.Screen(...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Plot Score Magnitude Comparison\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Compute score magnitudes\n",
    "network_mag = sqrt.(network_scores_u.^2 .+ network_scores_v.^2)\n",
    "gm_mag = sqrt.(gm_scores_u.^2 .+ gm_scores_v.^2)\n",
    "exact_mag = sqrt.(exact_scores_u.^2 .+ exact_scores_v.^2)\n",
    "\n",
    "# Common color scale based on exact score\n",
    "cmin, cmax = quantile(vec(exact_mag), [0.01, 0.99])\n",
    "\n",
    "fig_mag = Figure(size=(1800, 500))\n",
    "\n",
    "ax1 = Axis(fig_mag[1, 1], title=\"Network Score Magnitude\", \n",
    "           xlabel=\"x₁\", ylabel=\"x₂\", \n",
    "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
    "hm1 = heatmap!(ax1, x_range, y_range, network_mag, \n",
    "               colormap=:viridis, colorrange=(0, cmax))\n",
    "\n",
    "ax2 = Axis(fig_mag[1, 2], title=\"Gaussian Mixture Score Magnitude\", \n",
    "           xlabel=\"x₁\", ylabel=\"x₂\", \n",
    "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
    "hm2 = heatmap!(ax2, x_range, y_range, gm_mag, \n",
    "               colormap=:viridis, colorrange=(0, cmax))\n",
    "\n",
    "ax3 = Axis(fig_mag[1, 3], title=\"Exact Score Magnitude\", \n",
    "           xlabel=\"x₁\", ylabel=\"x₂\", \n",
    "           aspect=DataAspect(), limits=(-2, 2, -2, 2))\n",
    "hm3 = heatmap!(ax3, x_range, y_range, exact_mag, \n",
    "               colormap=:viridis, colorrange=(0, cmax))\n",
    "\n",
    "Colorbar(fig_mag[1, 4], hm3, label=\"Score Magnitude ||s(x)||\")\n",
    "\n",
    "display(fig_mag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Sampling with Langevin Dynamics\n",
    "\n",
    "Now we use our learned score to **generate new samples** from the distribution!\n",
    "\n",
    "### Langevin Dynamics\n",
    "\n",
    "Given the score function $\\mathbf{s}(\\mathbf{x})$, we can sample from $p(\\mathbf{x})$ using the **overdamped Langevin SDE**:\n",
    "\n",
    "$$d\\mathbf{x}_t = \\mathbf{s}(\\mathbf{x}_t) dt + \\sqrt{2} \\, d\\mathbf{W}_t$$\n",
    "\n",
    "Discretized with time step $\\Delta t$:\n",
    "\n",
    "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\Delta t \\cdot \\mathbf{s}(\\mathbf{x}_t) + \\sqrt{2 \\Delta t} \\cdot \\boldsymbol{\\xi}_t$$\n",
    "\n",
    "where $\\boldsymbol{\\xi}_t \\sim \\mathcal{N}(0, I)$.\n",
    "\n",
    "### Runge-Kutta Integration\n",
    "\n",
    "We use **4th-order Runge-Kutta (RK4)** for the deterministic part to improve accuracy:\n",
    "\n",
    "$$\\mathbf{k}_1 = \\mathbf{s}(\\mathbf{x}_t)$$\n",
    "$$\\mathbf{k}_2 = \\mathbf{s}(\\mathbf{x}_t + \\frac{\\Delta t}{2} \\mathbf{k}_1)$$\n",
    "$$\\mathbf{k}_3 = \\mathbf{s}(\\mathbf{x}_t + \\frac{\\Delta t}{2} \\mathbf{k}_2)$$\n",
    "$$\\mathbf{k}_4 = \\mathbf{s}(\\mathbf{x}_t + \\Delta t \\cdot \\mathbf{k}_3)$$\n",
    "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\frac{\\Delta t}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4) + \\sqrt{2 \\Delta t} \\cdot \\boldsymbol{\\xi}_t$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Langevin dynamics...\n",
      "  Samples: 1000, Steps: 1000, Δt: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%┣                                             ┫ 0/1.0k [00:00<00:00, -0s/it]\n",
      "0.1%┣                                         ┫ 1/1.0k [00:00<Inf:Inf, InfGs/it]\n",
      "1.0%┣▍                                           ┫ 10/1.0k [00:00<00:20, 50it/s]\n",
      "1.5%┣▋                                           ┫ 15/1.0k [00:00<00:16, 61it/s]\n",
      "2.4%┣█                                           ┫ 24/1.0k [00:00<00:12, 81it/s]\n",
      "3.3%┣█▌                                          ┫ 33/1.0k [00:00<00:10, 94it/s]\n",
      "3.9%┣█▊                                          ┫ 39/1.0k [00:00<00:10, 96it/s]\n",
      "4.8%┣██                                         ┫ 48/1.0k [00:00<00:09, 104it/s]\n",
      "5.4%┣██▎                                        ┫ 54/1.0k [00:01<00:09, 106it/s]\n",
      "6.3%┣██▊                                        ┫ 63/1.0k [00:01<00:08, 112it/s]\n",
      "7.1%┣███                                        ┫ 71/1.0k [00:01<00:08, 116it/s]\n",
      "7.8%┣███▍                                       ┫ 78/1.0k [00:01<00:08, 117it/s]\n",
      "8.6%┣███▊                                       ┫ 86/1.0k [00:01<00:08, 120it/s]\n",
      "9.3%┣████                                       ┫ 93/1.0k [00:01<00:08, 120it/s]\n",
      "10.2%┣████▏                                    ┫ 102/1.0k [00:01<00:07, 123it/s]\n",
      "11.1%┣████▌                                    ┫ 111/1.0k [00:01<00:07, 126it/s]\n",
      "11.8%┣████▉                                    ┫ 118/1.0k [00:01<00:07, 126it/s]\n",
      "12.7%┣█████▏                                   ┫ 127/1.0k [00:01<00:07, 128it/s]\n",
      "13.4%┣█████▌                                   ┫ 134/1.0k [00:01<00:07, 128it/s]\n",
      "14.3%┣█████▉                                   ┫ 143/1.0k [00:01<00:07, 130it/s]\n",
      "15.1%┣██████▏                                  ┫ 151/1.0k [00:01<00:06, 132it/s]\n",
      "15.7%┣██████▍                                  ┫ 157/1.0k [00:01<00:06, 131it/s]\n",
      "16.6%┣██████▉                                  ┫ 166/1.0k [00:01<00:06, 132it/s]\n",
      "17.3%┣███████                                  ┫ 173/1.0k [00:01<00:06, 132it/s]\n",
      "18.2%┣███████▌                                 ┫ 182/1.0k [00:01<00:06, 134it/s]\n",
      "19.1%┣███████▉                                 ┫ 191/1.0k [00:01<00:06, 135it/s]\n",
      "19.8%┣████████▏                                ┫ 198/1.0k [00:01<00:06, 134it/s]\n",
      "20.6%┣████████▌                                ┫ 206/1.0k [00:02<00:06, 135it/s]\n",
      "21.3%┣████████▊                                ┫ 213/1.0k [00:02<00:06, 134it/s]\n",
      "22.2%┣█████████                                ┫ 222/1.0k [00:02<00:06, 136it/s]\n",
      "23.1%┣█████████▌                               ┫ 231/1.0k [00:02<00:06, 136it/s]\n",
      "23.8%┣█████████▊                               ┫ 238/1.0k [00:02<00:06, 136it/s]\n",
      "24.7%┣██████████▏                              ┫ 247/1.0k [00:02<00:05, 137it/s]\n",
      "25.4%┣██████████▍                              ┫ 254/1.0k [00:02<00:05, 137it/s]\n",
      "26.3%┣██████████▉                              ┫ 263/1.0k [00:02<00:05, 138it/s]\n",
      "27.2%┣███████████▏                             ┫ 272/1.0k [00:02<00:05, 139it/s]\n",
      "27.9%┣███████████▍                             ┫ 279/1.0k [00:02<00:05, 138it/s]\n",
      "28.8%┣███████████▉                             ┫ 288/1.0k [00:02<00:05, 139it/s]\n",
      "29.4%┣████████████                             ┫ 294/1.0k [00:02<00:05, 138it/s]\n",
      "30.3%┣████████████▍                            ┫ 303/1.0k [00:02<00:05, 139it/s]\n",
      "31.1%┣████████████▊                            ┫ 311/1.0k [00:02<00:05, 140it/s]\n",
      "31.7%┣█████████████                            ┫ 317/1.0k [00:02<00:05, 139it/s]\n",
      "32.6%┣█████████████▍                           ┫ 326/1.0k [00:02<00:05, 140it/s]\n",
      "33.3%┣█████████████▋                           ┫ 333/1.0k [00:02<00:05, 139it/s]\n",
      "34.2%┣██████████████                           ┫ 342/1.0k [00:02<00:05, 140it/s]\n",
      "35.1%┣██████████████▍                          ┫ 351/1.0k [00:02<00:05, 141it/s]\n",
      "35.8%┣██████████████▊                          ┫ 358/1.0k [00:03<00:05, 140it/s]\n",
      "36.7%┣███████████████                          ┫ 367/1.0k [00:03<00:04, 141it/s]\n",
      "37.3%┣███████████████▎                         ┫ 373/1.0k [00:03<00:04, 140it/s]\n",
      "38.2%┣███████████████▋                         ┫ 382/1.0k [00:03<00:04, 141it/s]\n",
      "39.0%┣████████████████                         ┫ 390/1.0k [00:03<00:04, 141it/s]\n",
      "39.6%┣████████████████▎                        ┫ 396/1.0k [00:03<00:04, 141it/s]\n",
      "40.5%┣████████████████▋                        ┫ 405/1.0k [00:03<00:04, 141it/s]\n",
      "41.3%┣█████████████████                        ┫ 413/1.0k [00:03<00:04, 141it/s]\n",
      "42.2%┣█████████████████▎                       ┫ 422/1.0k [00:03<00:04, 141it/s]\n",
      "43.1%┣█████████████████▊                       ┫ 431/1.0k [00:03<00:04, 142it/s]\n",
      "43.8%┣██████████████████                       ┫ 438/1.0k [00:03<00:04, 141it/s]\n",
      "44.7%┣██████████████████▎                      ┫ 447/1.0k [00:03<00:04, 142it/s]\n",
      "45.4%┣██████████████████▋                      ┫ 454/1.0k [00:03<00:04, 142it/s]\n",
      "46.3%┣███████████████████                      ┫ 463/1.0k [00:03<00:04, 142it/s]\n",
      "47.2%┣███████████████████▍                     ┫ 472/1.0k [00:03<00:04, 142it/s]\n",
      "47.9%┣███████████████████▋                     ┫ 479/1.0k [00:03<00:04, 142it/s]\n",
      "48.8%┣████████████████████                     ┫ 488/1.0k [00:03<00:04, 143it/s]\n",
      "49.5%┣████████████████████▎                    ┫ 495/1.0k [00:03<00:04, 143it/s]\n",
      "50.4%┣████████████████████▋                    ┫ 504/1.0k [00:04<00:03, 143it/s]\n",
      "51.3%┣█████████████████████                    ┫ 513/1.0k [00:04<00:03, 143it/s]\n",
      "52.2%┣█████████████████████▍                   ┫ 522/1.0k [00:04<00:03, 143it/s]\n",
      "53.1%┣█████████████████████▊                   ┫ 531/1.0k [00:04<00:03, 143it/s]\n",
      "53.8%┣██████████████████████                   ┫ 538/1.0k [00:04<00:03, 143it/s]\n",
      "54.7%┣██████████████████████▍                  ┫ 547/1.0k [00:04<00:03, 144it/s]\n",
      "55.4%┣██████████████████████▊                  ┫ 554/1.0k [00:04<00:03, 143it/s]\n",
      "56.3%┣███████████████████████                  ┫ 563/1.0k [00:04<00:03, 144it/s]\n",
      "57.2%┣███████████████████████▌                 ┫ 572/1.0k [00:04<00:03, 144it/s]\n",
      "57.9%┣███████████████████████▊                 ┫ 579/1.0k [00:04<00:03, 144it/s]\n",
      "58.8%┣████████████████████████                 ┫ 588/1.0k [00:04<00:03, 144it/s]\n",
      "59.5%┣████████████████████████▍                ┫ 595/1.0k [00:04<00:03, 144it/s]\n",
      "60.4%┣████████████████████████▊                ┫ 604/1.0k [00:04<00:03, 144it/s]\n",
      "61.3%┣█████████████████████████▏               ┫ 613/1.0k [00:04<00:03, 144it/s]\n",
      "62.2%┣█████████████████████████▌               ┫ 622/1.0k [00:04<00:03, 144it/s]\n",
      "63.1%┣█████████████████████████▉               ┫ 631/1.0k [00:04<00:03, 145it/s]\n",
      "63.8%┣██████████████████████████▏              ┫ 638/1.0k [00:04<00:03, 144it/s]\n",
      "64.7%┣██████████████████████████▌              ┫ 647/1.0k [00:04<00:02, 145it/s]\n",
      "65.3%┣██████████████████████████▊              ┫ 653/1.0k [00:05<00:02, 144it/s]\n",
      "66.2%┣███████████████████████████▏             ┫ 662/1.0k [00:05<00:02, 145it/s]\n",
      "67.1%┣███████████████████████████▌             ┫ 671/1.0k [00:05<00:02, 145it/s]\n",
      "67.8%┣███████████████████████████▉             ┫ 678/1.0k [00:05<00:02, 145it/s]\n",
      "68.7%┣████████████████████████████▏            ┫ 687/1.0k [00:05<00:02, 145it/s]\n",
      "69.4%┣████████████████████████████▌            ┫ 694/1.0k [00:05<00:02, 145it/s]\n",
      "70.3%┣████████████████████████████▉            ┫ 703/1.0k [00:05<00:02, 145it/s]\n",
      "71.2%┣█████████████████████████████▏           ┫ 712/1.0k [00:05<00:02, 145it/s]\n",
      "71.8%┣█████████████████████████████▍           ┫ 718/1.0k [00:05<00:02, 145it/s]\n",
      "72.7%┣█████████████████████████████▉           ┫ 727/1.0k [00:05<00:02, 145it/s]\n",
      "73.3%┣██████████████████████████████           ┫ 733/1.0k [00:05<00:02, 145it/s]\n",
      "74.2%┣██████████████████████████████▍          ┫ 742/1.0k [00:05<00:02, 145it/s]\n",
      "75.0%┣██████████████████████████████▊          ┫ 750/1.0k [00:05<00:02, 145it/s]\n",
      "75.7%┣███████████████████████████████          ┫ 757/1.0k [00:05<00:02, 145it/s]\n",
      "76.6%┣███████████████████████████████▍         ┫ 766/1.0k [00:05<00:02, 145it/s]\n",
      "77.3%┣███████████████████████████████▊         ┫ 773/1.0k [00:05<00:02, 145it/s]\n",
      "78.2%┣████████████████████████████████         ┫ 782/1.0k [00:05<00:01, 145it/s]\n",
      "79.1%┣████████████████████████████████▍        ┫ 791/1.0k [00:05<00:01, 146it/s]\n",
      "79.8%┣████████████████████████████████▊        ┫ 798/1.0k [00:05<00:01, 145it/s]\n",
      "80.7%┣█████████████████████████████████        ┫ 807/1.0k [00:06<00:01, 146it/s]\n",
      "81.3%┣█████████████████████████████████▎       ┫ 813/1.0k [00:06<00:01, 145it/s]\n",
      "82.2%┣█████████████████████████████████▊       ┫ 822/1.0k [00:06<00:01, 146it/s]\n",
      "83.1%┣██████████████████████████████████       ┫ 831/1.0k [00:06<00:01, 146it/s]\n",
      "83.8%┣██████████████████████████████████▍      ┫ 838/1.0k [00:06<00:01, 146it/s]\n",
      "84.7%┣██████████████████████████████████▊      ┫ 847/1.0k [00:06<00:01, 146it/s]\n",
      "85.4%┣███████████████████████████████████      ┫ 854/1.0k [00:06<00:01, 146it/s]\n",
      "86.3%┣███████████████████████████████████▍     ┫ 863/1.0k [00:06<00:01, 146it/s]\n",
      "87.2%┣███████████████████████████████████▊     ┫ 872/1.0k [00:06<00:01, 146it/s]\n",
      "87.9%┣████████████████████████████████████     ┫ 879/1.0k [00:06<00:01, 146it/s]\n",
      "88.8%┣████████████████████████████████████▍    ┫ 888/1.0k [00:06<00:01, 146it/s]\n",
      "89.4%┣████████████████████████████████████▋    ┫ 894/1.0k [00:06<00:01, 146it/s]\n",
      "90.3%┣█████████████████████████████████████    ┫ 903/1.0k [00:06<00:01, 146it/s]\n",
      "91.1%┣█████████████████████████████████████▍   ┫ 911/1.0k [00:06<00:01, 146it/s]\n",
      "91.7%┣█████████████████████████████████████▋   ┫ 917/1.0k [00:06<00:01, 146it/s]\n",
      "92.6%┣██████████████████████████████████████   ┫ 926/1.0k [00:06<00:01, 146it/s]\n",
      "93.3%┣██████████████████████████████████████▎  ┫ 933/1.0k [00:06<00:00, 146it/s]\n",
      "94.2%┣██████████████████████████████████████▋  ┫ 942/1.0k [00:06<00:00, 146it/s]\n",
      "95.1%┣███████████████████████████████████████  ┫ 951/1.0k [00:07<00:00, 146it/s]\n",
      "95.8%┣███████████████████████████████████████▎ ┫ 958/1.0k [00:07<00:00, 146it/s]\n",
      "96.7%┣███████████████████████████████████████▋ ┫ 967/1.0k [00:07<00:00, 146it/s]\n",
      "97.4%┣████████████████████████████████████████ ┫ 974/1.0k [00:07<00:00, 146it/s]\n",
      "98.3%┣████████████████████████████████████████▎┫ 983/1.0k [00:07<00:00, 146it/s]\n",
      "99.2%┣████████████████████████████████████████▊┫ 992/1.0k [00:07<00:00, 146it/s]\n",
      "\u001b[1A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99.9%┣█████████████████████████████████████████┫ 999/1.0k [00:07<00:00, 146it/s]\n",
      "100.0%┣███████████████████████████████████████┫ 1.0k/1.0k [00:07<00:00, 146it/s]\n",
      "100.0%┣███████████████████████████████████████┫ 1.0k/1.0k [00:07<00:00, 146it/s]\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Sampling from the Learned Score Field\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# Use network as our score function\n",
    "∇V(x) = predict(network, x)\n",
    "\n",
    "# Langevin dynamics parameters\n",
    "ϵ = sqrt(2)    # Noise coefficient (√2 for standard Langevin)\n",
    "Nₜ = 1000      # Number of time steps\n",
    "Nₑ = 1000      # Number of ensemble members (parallel chains)\n",
    "Δt = 0.1       # Time step size\n",
    "\n",
    "# Initialize from standard Gaussian\n",
    "x₀ = randn(2, Nₑ)\n",
    "\n",
    "println(\"Running Langevin dynamics...\")\n",
    "println(\"  Samples: $(Nₑ), Steps: $(Nₜ), Δt: $(Δt)\")\n",
    "\n",
    "for t in ProgressBar(1:Nₜ)\n",
    "    𝒩 = randn(2, Nₑ)  # Fresh noise for all samples\n",
    "    \n",
    "    for ω in 1:Nₑ\n",
    "        x = x₀[:, ω]\n",
    "        \n",
    "        # Runge-Kutta 4 stages for deterministic drift\n",
    "        k1 = ∇V(x)\n",
    "        k2 = ∇V(x + Δt / 2 * k1)\n",
    "        k3 = ∇V(x + Δt / 2 * k2)\n",
    "        k4 = ∇V(x + Δt * k3)\n",
    "        \n",
    "        # RK4 update (deterministic part)\n",
    "        x₀[:, ω] .= x + Δt / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "        \n",
    "        # Add stochastic noise (Brownian motion)\n",
    "        x₀[:, ω] .+= ϵ * √Δt * 𝒩[:, ω]\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"\\nSampling complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5: Generated Samples vs Original Data\n",
    "\n",
    "This figure compares the **marginal distributions** of our generated samples (blue) with the original training data (red).\n",
    "\n",
    "**What to observe:**\n",
    "- **Mode locations**: Generated samples should cluster around $x \\approx \\pm 1$\n",
    "- **Mode weights**: The relative heights of the two peaks should match\n",
    "- **Shape**: The overall distribution shape should be similar\n",
    "- **Quality indicator**: Good overlap means the network learned the distribution well\n",
    "\n",
    "**Common issues:**\n",
    "- Missing modes: Network didn't capture all peaks\n",
    "- Mode collapse: Samples cluster at single peak\n",
    "- Spread mismatch: Wrong variance indicates poor score magnitude learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GLMakie.Screen(...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Compare Generated Samples with Original Data\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "binsize = 30\n",
    "\n",
    "fig_samples = Figure(size=(900, 400))\n",
    "\n",
    "ax1 = Axis(fig_samples[1, 1], title=\"x₁ Marginal Distribution\", \n",
    "           xlabel=\"x₁\", ylabel=\"Density\")\n",
    "hist!(ax1, x₀[1, :], bins=binsize, normalization=:pdf, \n",
    "      color=(:steelblue, 0.6), label=\"Generated\")\n",
    "hist!(ax1, data[1, :], bins=binsize, normalization=:pdf, \n",
    "      color=(:crimson, 0.4), label=\"Original\")\n",
    "axislegend(ax1, position=:rt)\n",
    "\n",
    "ax2 = Axis(fig_samples[1, 2], title=\"x₂ Marginal Distribution\", \n",
    "           xlabel=\"x₂\", ylabel=\"Density\")\n",
    "hist!(ax2, x₀[2, :], bins=binsize, normalization=:pdf, \n",
    "      color=(:steelblue, 0.6), label=\"Generated\")\n",
    "hist!(ax2, data[2, :], bins=binsize, normalization=:pdf, \n",
    "      color=(:crimson, 0.4), label=\"Original\")\n",
    "axislegend(ax2, position=:rt)\n",
    "\n",
    "display(fig_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STATISTICAL COMPARISON: Generated vs Original\n",
      "============================================================\n",
      "\n",
      "Mean:\n",
      "  Generated: [-0.146, 0.1305]\n",
      "  Original:  [-0.0119, 0.0008]\n",
      "\n",
      "Covariance (Generated):\n",
      "  [1.2442  -0.4787]\n",
      "  [-0.4787  1.3027]\n",
      "\n",
      "Covariance (Original):\n",
      "  [1.2342  -0.3766]\n",
      "  [-0.3766  1.2231]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Statistical Comparison\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "println(\"=\"^60)\n",
    "println(\"STATISTICAL COMPARISON: Generated vs Original\")\n",
    "println(\"=\"^60)\n",
    "println()\n",
    "println(\"Mean:\")\n",
    "println(\"  Generated: \", round.(vec(mean(x₀, dims=2)), digits=4))\n",
    "println(\"  Original:  \", round.(vec(mean(data, dims=2)), digits=4))\n",
    "println()\n",
    "println(\"Covariance (Generated):\")\n",
    "cov_gen = cov(x₀, dims=2)\n",
    "println(\"  [\", round(cov_gen[1,1], digits=4), \"  \", round(cov_gen[1,2], digits=4), \"]\")\n",
    "println(\"  [\", round(cov_gen[2,1], digits=4), \"  \", round(cov_gen[2,2], digits=4), \"]\")\n",
    "println()\n",
    "println(\"Covariance (Original):\")\n",
    "cov_orig = cov(data, dims=2)\n",
    "println(\"  [\", round(cov_orig[1,1], digits=4), \"  \", round(cov_orig[1,2], digits=4), \"]\")\n",
    "println(\"  [\", round(cov_orig[2,1], digits=4), \"  \", round(cov_orig[2,2], digits=4), \"]\")\n",
    "println(\"=\"^60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Score functions** $\\mathbf{s}(\\mathbf{x}) = \\nabla \\log p(\\mathbf{x})$ encode the gradient of log-probability\n",
    "\n",
    "2. **Denoising Score Matching** provides a practical training objective:\n",
    "   $$\\mathcal{L}_{\\text{DSM}} = \\mathbb{E}\\left[ \\left\\| \\mathbf{s}_\\theta(\\mathbf{x} + \\sigma\\mathbf{z}) + \\frac{\\mathbf{z}}{\\sigma} \\right\\|^2 \\right]$$\n",
    "\n",
    "3. **Neural networks** can approximate score functions effectively\n",
    "\n",
    "4. **Langevin dynamics** enables sampling using only the score function\n",
    "\n",
    "### Extensions\n",
    "\n",
    "This tutorial covers the basics. Modern score-based models extend these ideas:\n",
    "\n",
    "- **Multiple noise levels**: Train on a range of $\\sigma$ values (Song & Ermon, 2019)\n",
    "- **Continuous diffusion**: Use SDEs with time-varying noise (Song et al., 2021)\n",
    "- **Conditional generation**: Add conditioning variables for controlled generation\n",
    "- **Latent diffusion**: Work in compressed latent spaces (Rombach et al., 2022)\n",
    "\n",
    "### References\n",
    "\n",
    "- Vincent, P. (2011). \"A Connection Between Score Matching and Denoising Autoencoders\"\n",
    "- Song, Y., & Ermon, S. (2019). \"Generative Modeling by Estimating Gradients of the Data Distribution\"\n",
    "- Song, Y., et al. (2021). \"Score-Based Generative Modeling through Stochastic Differential Equations\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
